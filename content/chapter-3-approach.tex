% !TEX root = ../my-thesis.tex
%
\chapter{AutoML with an Ensemble of Optimizers}
\label{sec:approach}
As outlined in the previous chapter, in theory any optimization approach under a resource budget is limited to a set of problem classes, where it can find a solution that is better than any solution another optimization approach could find for the same problem class, i.e. where the approach is superior.
For an actual use-case, as for instance AutoML, it would be necessary to either have significant domain knowledge or to had conducted a broad empirical evaluation to select the optimizer that have a high chance of performing superior for a newly encountered problem class.
This can be avoided, if during the optimization process a set of optimizers would be evaluated in the context of the presented optimization problem and the most apt one would be applied automatically.\newline
Furthermore, it was argued how a separation of the optimization space into model selection and model configuration spaces by optimization space transformations could be beneficial.
The anticipated advantages are a speed-up at first, because the dimensionality of the input space for an optimizer can be reduced.
Secondly, the best suited optimizer for the optimization space of the model configuration of a constructed pipeline can be selected independently for each different result of the model selection steps.

Although this approach could be generalized to any form of algorithm selection and hyperparameter configuration problem, this thesis focusses on the AutoML use-case and therewith has to address well suited machine learning pipelines as an expected output for a wide variety of datasets as an input.
Thus, the constructed pipelines should be able to be as sophisticated as necessary to suit the complexity of any possible input dataset.
Therewith, the approach has the additional requirement of preventing overly stringent limitations of the pipeline topology such as a fix length or solely linear compositions.\newline
In the following chapter such a method will be presented and applied to the AutoML use-case.
This approach will be explained in the following steps in separate sections:
\begin{enumerate}
    \item A short overview how model selection and configuration spaces can be separated for this approach
    \item The structure of the model selection optimization space and how the optimization will be performed to enable the selection of different optimizers for model configuration out of an embedded optimizer ensemble
    \item The structure of the model configuration optimization space and which benefits can be utilized from using and re-using the different optimizers of the ensemble
\end{enumerate}

\section{Separation of Model Selection and Configuration}
\label{sec:approach:separation}
A usual AutoML optimization search space is a joint space for model selection and model configuration consisting out of a set of components, which can be used to construct a machine learning pipeline, and based on the selected components, a valid configuration has to be created.
Since in the joint optimization space the selected components are not known beforehand, the dimensionality of this space has to be selected for a fix parametrization size.
Hence, the model configuration can only create parameter configurations with a pre-defined size as a constraint and the set of pipeline components for model selection can only contain components whose configuration does not exceed this size.\newline
If the model selection and the model configuration steps are performed sequentially on separated optimization spaces, the dimensionality of the model configuration space can depend on the outcome of the model selection step and have the necessary dimensionality for the constructed pipelines parametrization.\newline
Similar to ReinBo and Mosaic, the model selection space is represented by a tree structure.
After the optimization method reaches a leaf node and has therewith complete and valid pipeline structure, the size of the required configuration becomes clear.
Any leaf node is for this reason the connection point and foundation for a model configuration space and the configuration requirements can be deduced.
Thus, dimensionality and structure of the optimization space for the model configuration can be determined from the configuration requirements of the pipeline represented in this leaf node.

\section{Model Selection with MCTS}
\label{sec:approach:selection}
While the overall goal of the model selection step is to construct a pipeline out of a set of pipeline components, in this approach two additional sub-goals come in addition as supplementary requirements:
\begin{enumerate}
    \item The constructed pipeline should be able to be constructed more flexible and more sophisticated than in ReinBo and Mosaic, i.e. have an unrestricted length and no constraints regarding linearity.
    \item The model selection step should be able to evaluate the different optimization methods and their performance regarding the input dataset or even regarding the model configuration for specific pipeline components in the context in the dataset.
    With such an approach, the the most suitable optimizer can be detected and exploited, which will be the key concept for creating an optimizer ensemble.
\end{enumerate}
In the following, both sub-goals will be addressed.
The next two sub-sections tackle the first sub-goal by modelling the model selection process as a graph search problem as first and describing this search graph in more detail afterwards.
Thereafter, in the third sub-section a formalization of the ensemble concept for multiple optimizers is given in the form of a Multi-Armed Bandit problem.
Finally, in the fourth sub-section it is illustrated how both sub-goals can be achieved during the model selection by performing a MCTS.

\subsection{Model Selection as a Graph Search Problem}
\label{sec:appraoch:selection:search}
A machine learning pipeline can vary heavily in structure and complexity.
For example, simple using a Decision tree as the only component as well as many components in a more sophisticated topology as in figure~\ref{fig:appraoch:complex-pipeline} are valid pipelines and depending on the concrete input dataset, both variants could be the optimal output of the model selection step.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=\textwidth]{gfx/Figures/Approach/ComplexPipeline.pdf}
    \caption{An example of a complex machine learning pipeline with a higher length and a non-linear topology. Input dataset and output prediction data are shown in green, pre-processing components in orange and the machine learning components constructing the machine learning model in blue.}
    \label{fig:appraoch:complex-pipeline}
\end{figure}
Since ReinBo and Mosaic solve their model selection tasks by defining a fix pipeline length and creating a model selection tree containing every combinatorial possible combination from a few different component types, these approaches do not allow complex pipelines.\newline
Other approaches, as for example ML-Plan, TPOT or RECIPE utilize hierarchical task network planning (\textit{HTN planning}) combined with a best-first search or expression trees and formal grammars together with genetic programming, to achieve a more unconstrained model selection optimization space.
Because for the ensemble method a MCTS will be required (will be reasoned in~\ref{sec:appraoch:selection:mcts}), which is a heuristic search algorithm like the best-first search as in ML-Plan, this approach will take this HTN planning approach as a foundation to construct the model selection space in the form of a tree.

As the name suggests, HTN planning is based around the notion of tasks and usually one goal task is given as the planning problem input, which would be "\textit{Construct a machine learning pipeline out of a set of components}" in this case.
These tasks can either be primitive tasks or compound task, where the overall goal task usually is a compound task.
Primitive tasks are simple enough be directly achievable without the need for further planning or other calculations.
An example for a primitive task in the AutoML context could be "\textit{Use a Decision tree as a learning model for the pipeline}" as this task can be directly transformed into a pipeline construction command.\newline
Compound tasks on the other hand, are not directly realizable and it is necessary to further decompose the task.
Each compound task can be decomposed into one or most commonly two or more sub-tasks.
Depending on the planning domain, there are decomposition rules where a compound task has one or more possible decompositions.
Such sub-tasks of a compound task can be primitive tasks, again compound tasks or a mixture of both.
If a compound task can be decomposed into solely primitive tasks, this task is directly solvable via the actions of the primitive tasks.
But if the decomposition of the compound task consists out of at least one compound tasks, this child compound task needs to be decomposed recursively as well until it becomes solvable and this solution can be used to solve the original compound task.
The exemplaric AutoML compound tasks "\textit{Construct a machine learning pipeline out of a set of components}" could be decomposed for example into the to smaller compound tasks "\textit{Construct a pre-processing pipeline out of a set of pre-processing components}" and "\textit{Construct a learning model out of a set of machine learning components}".

With this decomposition of compound tasks into different sub-tasks, the hierarchical aspect of HTN planning is added.
A plan as the solution of the planning problem is therefore a task-tree, where each leaf node is a primitive task and each inner node is a compound task.
To create a search tree for a HTN planning problem, solution states, i.e. solved or incomplete task-trees, as well as planning operations to solve an incomplete task-tree, i.e. selecting and applying decomposition rules, need to be represented with nodes and edges.\newline
Here is one common possibility to have each node represent a task-tree and each outgoing edge is a decomposition rule that is applicable to the task-tree of the node.
This edges are therefore connected to nodes, where the represented task-tree is the result of applying this decomposition rule on the previous task-tree.
For a bigger HTN problem an unsolved task-tree will contain a high number of undecomposed compound nodes and therefore an immense amount of decomposition rules would be applicable.
To reduce the maximum degree of the graph and therefore the memory and runtime complexity of most search algorithms, there should not be an outgoing edge for the applicable decomposition rules of each undecomposed compound task.
A simplification is to create an ordering for the undecomposed compound task of the nodes task tree and only create outgoing edges for decomposition rules, which are applicable to the first compound task of this ordering.
The representation of one simple incomplete task tree and two applicable decomposition rules for the first compound task of an ordering is illustrated in figure~\ref{fig:appraoch:htn-planning}.
\begin{figure}[ht!]
    \centering
    \includegraphics[height=0.5\textheight]{gfx/Figures/Approach/HTN.pdf}
    \caption{An example of a partial HTN planning search tree for a simple planning problem.
    Each node is illustrated with the task-tree for the partially solved plan up to this point in its upper half, where compound task nodes are blue and primitive task nodes are green.
    In the lower half are all involved compound tasks listed in the utilized ordering.
    If any decomposition rule was already applied for a compound task, it is listed here as well.
    The first node on the left has already utilized a decomposition for compound task $c_1$ into $c_2$ and $c_3$.
    In this ordering the first undecomposed compound task is $c_2$ and since two decomposition rules $c_2 \rightarrow (c_4, p_1)$ and $c_2 \rightarrow (p_2, p_3)$ are applicable, the node of this task-tree has two outgoing edges to nodes, where the corresponding decomposition rule was applied to the represented task tree.
    After applying the second decomposition rule, $c_2$ has solely primitive tasks as sub-tasks and can therefore be marked as solved.
    In the case of the first decomposition rule, $c_4$ needs to be decomposed and solved before $c_2$ can would be solved.
    }
    \label{fig:appraoch:htn-planning}
\end{figure}

\subsection{Description of the Search Space Graph}
\label{sec:appraoch:selection:graph}
The examples of HTN tasks in~\ref{sec:appraoch:selection:search} are given as instructions in plain english but this cannot be used in a algorithm and therefore a formalization is necessary.
With a suitable formalization and corresponding data model, the HTN planning domain is adjustable and expandable for the exact use-case and for the case of the AutoML context it is possible to modify possible pipeline topologies and the set of pipeline components that are used for construction.\newline
In a manual pipeline construction it is not possible to create arbitrary pipeline graphs because components often expect a certain type of input that can only be given by certain other components as an output.
Additionally, the selection of some components implies the requirement of selecting certain other components.
For example if an ensemble method was selected as a learning model, the components that are used to be the predictors of the ensemble must be learning model components and cannot be pre-processing models.

To incorporate such constraints into tasks and decomposition rules, ML-Plan utilizes a simple type system in the form of required and provided interfaces.
Each task has one or more type definitions which represents certain properties of the solution to the associated task.
For example, the primitive task of using a Decision Tree as well as the compound task of creating an ensemble learning model out of several other learning models would both offer a solution in the form of a machine learning model.
Both of them could have something like \texttt{Classifier} or \texttt{Learner} as their solution type and therewith as the types of interfaces they provide.\newline
On the other hand, are compound tasks only solvable if they get solutions of certain types as the results of the sub-tasks they are decomposed into.
For example, a Stacking classifier can only be based other classifiers, i.e. solutions with types like \texttt{Classifier}.
Thus, each compound tasks has one or more required interface, which represents this solution types the compound task will be based on.\newline
Decomposition rules are now simple matching rules, i.e. which required interfaces can be satisfied with which provided interface.
Of course it is possible that this interface types need to be identically for simplicity.
A compound task with a required interface of type $a$ can only be decomposed into another task, which has $a$ as one of its provided interface types.
With such a definitions of required and provided interface types for each task, pipeline topology specific constraints like splits into two sub-pipelines and uniting such sub-pipelines, or machine learning specific constraints like for example creating ensembles out of classifiers, are straightforward to incorporate into the planning problem without the need to write a list of decomposition rules.\newline
The overall pipeline structure, for example Pre-Processing + Learning Model or Pre-Processing + Learning Model + Post-Processing, can be encoded in the required interfaces of the goal task.
Every actual pipeline component, which shall be included for constructing pipelines, will be represented as primitive tasks.
For a better structure, they should be grouped by their provided interface types that are deduced from their properties and purposes.
Pipeline topologies, like creating parallel pre-processing sub-pipelines can be for example achieved with a Feature Union pipeline component, which has one or more types of pre-processing components as required interfaces.
Pipelines with a dynamic length can be achieved comparable to generating sequences with an unlimited length in a regular grammar, where a pipeline can either be decomposed into just a component or a component and a pipeline.
A simple HTN planning space for a very small AutoML model selection space is illustrated in~\ref{fig:appraoch:htn-automl}.
For a better understanding, an example of a more detailed and sophisticated AutoML HTN planning space is given in Appendix~\ref{sec:appendix:htn-space}.
It is written in the JSON schema, which is defined in~\ref{sec:implementation:json}.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=\textwidth]{gfx/Figures/Approach/HTNAutoML.pdf}
    \caption{A complete search graph of an AutoML problem visualized in the HTN context.
    The primitive tasks are PCA, ICA and SVN.
    All available compound tasks are Pipeline, Pre-Processor, Classifier, Feature Union, First Pre-Processor and Second Pre-Processor.}
    \label{fig:appraoch:htn-automl}
\end{figure}

\subsection{Multiple Optimization Algorithms as a Multi-Armed Bandit Problem}
\label{sec:appraoch:selection:bandit}
All nodes, where every compound tasks is solved, are therefore leaf nodes and all of them together will be grouped in $N^*$.
Each $n \in N^*$ has a completely constructed pipeline $p_n \in P$ represented with the internal task-tree of $n$, where $P$ is the set of all possible pipelines.
For this $p$, it can be collected which parameters each included component required and therewith a optimization space for the model configuration can be deduced.\newline
The approach requires a set of optimization algorithms, or alternatively differently configured variants of the one optimization algorithm, as an input.
This forms the optimizer algorithms set  $A = \{ a_1, ..., a_k \}$.
To enable the actual model configuration with one of the available optimization algorithms, one additional node for each $a_i \in A$ will be attached as a child to each model selection leaf node $n_j \in N^*$ in the form of $n_j^{a_i}$.
All of these additional nodes for optimization that are child nodes of the model selection leaf nodes $N^*$ are in the node set $N^A = {n^{a_1}_{n^*_1}, ... , n^{a_k}_{n^*_l}}$.
A node with an optimization algorithm $n^{a_i}_{n^*_j} \in N^A$ represents the node that is attached as a child to the model selection leaf node $n^*_j$ and uses the optimization algorithm $a_i$ to optimize a configuration for pipeline $p_{n^*_j}$.\newline
Therefore, the search tree $G=(V, E)$ of the model selection HTN planning is extended for $N^A$ in the form of $V'=V \cup N^A$, $E'=E \cup \bigcup_{n^*_i \in N^*} \bigcup_{a_j \in A} n^{a_j}_{n^*_i}$, $G' = (V', E')$.
Since the nodes in $N^*$ are now not longer leaf nodes of $G'$ they will now be referred to as \textit{Pipeline Nodes} and the newly added leaf nodes $N^A$ are the \textit{Optimizer Nodes.}
An simplified illustration of a possible $G'$ without the HTN planning aspects of this extended search tree can be seen in figure~\ref{fig:appraoch:search-graph}.
The pipeline nodes and the optimizer nodes are also marked for a better intuition of the extended search tree $G'$.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=\textwidth]{gfx/Figures/Approach/SearchGraph.pdf}
    \caption{A simplified illustration of a possible extended search tree $G'$.
    The root is drawn in blue, the inner nodes with unsolved task-trees in white, pipelines nodes in green and optimizer nodes in red.
    The yellow box shows the configuration space of the pipeline that is represented by the task tree in that pipeline node.}
    \label{fig:appraoch:search-graph}
\end{figure}

Now, if a pipeline node is reached, the task arises to select one of the connected optimizer nodes.
The set of optimizer nodes below a pipeline node $n^*_j$ forms the set $N^A_{n^*_j} = \bigcup_{a_i \in A} n^{a_i}_{n^*_j}$.
But without prior knowledge it is impossible to reliably predict, which optimizer is ideally suited to optimize the configuration for the pipeline $p_{n^*_j}$, i.e. to select one node out of $N^A_{n^*_j}$ during a graph search.\newline
A na\"ive approach is to randomly try out different optimization algorithms for optimizing the parametrization and examine their optimization results to gather knowledge successively.
This try-and-error approach is also referred to as \textit{exploration}.
During the collection of exploration results it will become more evident, which optimizer yields which score quality.\newline
Collecting a few sample scores for each optimizer is basically conducting an experiment since black-box optimization is often a probabilistic process.
Based on the amount of experiment samples, the of optimization capability an optimizer can be estimated with a certain probability.
After a sufficient amount of samples is gathered, the most suitable optimizer can be selected and utilized with a high certainty.
Using one choice with the expectation of a good result is also named \textit{exploitation}.\newline
The balance between both, exploration and exploitation, is a difficult challenge because of the optimization budget.
With too much exploitation it can not be assured with a high probability that the exploited choice is the actual best one, since there is a lack of optimization score samples.
But with too much exploration it may be possible to determine the probably best choice, but there is not enough budget left to actually utilize this choice to a higher extend.

This balance between exploring and exploiting a set of choices, which have an unknown quality, is fundamental problem of the \textit{Multi-Armed Bandit} problem.
A multi-armed bandit problem is usually considered in the context of time steps $t_1, t_2, .., T$, which are sometimes also called turns, where $T$ is the current turn of the ongoing series.
For every turn one arm, i.e. one of the $K$ choices $C = \langle c_1, ..., c_K \rangle$, is selected by a selection strategy $s: t \rightarrow [1,K]$.\newline
For a turn $t$ a random reward $r$ is received for selecting the choice $c_{s(t)}$.
Each arm $c_i$ has an underlying probably distribution $D_{c_i}$ for the reward values, i.e. $r_{c_{s(t)}} \sim D_{c_{s(t)}}$.
In the AutoML context, where each arm is one optimizer, this reward could be the test score a pipeline, which is configured with the best found parametrization of the chosen optimizer after a certain budget.\newline
Of course, each distribution $D_{c_i}$ has a corresponding expected value $\mu_{c_i}$.
Therefore, if for example the selection strategy is $s(t) = 1$ the accumulated expected reward is $\sum_{t=1}^T = \mu_{c_1}$.\newline
An optimal strategy $s^*$ would always pick the choice with the highest expected reward $\mu^* = \underset{i \in [1,K]}{\mathrm{max}} \> \mu_{c_i}$, such that $s^*(t) =\underset{i \in [1,K]}{\mathrm{argmax}} \> \mu_{c_i}$.
However, $s^*$ is unknown and has to be approximated by $s$ over time.\newline
For every turn where $s(t) \neq s^*(t)$, the strategy looses $\mu^* - \mu_{c_{s(t)}}$ in expected reward.
The exploration vs. exploitation balance problem is here noticeable, since the strategy has to find out the arm with $\mu^*$ by trying out every arm, but also wants to keep $\mu^* - \mu_{c_{s(t)}}$ as minimal as possible for every turn.
This idea of keeping the expected reward loss minimal in the overall context of $\underset{T \rightarrow \infty}{\lim}$, such that the strategy has time to approximate $s^*$, is formulated in the form of the \textit{Total expected Regret}: $R_T = T \cdot \mu^* - \sum_{t=1}^{T} \mu_{c_{s(t)}}$.
Different algorithms for creating such selection strategies $s$ can be compared by their respective expected total regret $R_T$, which should be as low as possible.

\subsection{Ensemble Interaction with a MCTS}
\label{sec:appraoch:selection:mcts}
The selection of an optimizer node out of $N^A_{n^*_j}$ to perform the model configuration of the pipeline $p_{n^*_j}$ can be formalized as a multi-armed bandit problem.
With that assumption, there are several existing algorithms to construct selection strategies for optimization nodes until the optimization budget is spent, which balance exploration and exploitation with the goal of keeping the total expected regret minimal.
Common choices are for example \textit{Thompson Sampling}~\cite{Thompson-Sampling} or the usage of \textit{Upper confidence bounds} (UCB) as for instance in the \textit{UCB1} algorithm~\cite{Auer-UCB1}.\newline
With this very limited set of arms, one of this algorithms could be applied and with a high enough optimization budget, there is a high probability that the best suited one will be exploited enough to get a good parametrization for $p_{n^*_j}$.
But since no actual model selection is done until now, there is no pipeline $p_{n^*_j}$ selected and therefore the set of arms cannot be limited to $N^A_{n^*_j}$.
Instead, the set of arms would be the set of optimizer nodes $N^A$, i.e. the set of combinations of all possible pipelines with all algorithms.\newline
For such a high number of arms, the multi-armed bandit algorithm would need a massive amount of steps, where exploration would be performed, until exploitation could take place.
As an alternative, the tree structure of $G'$ should be utilized for a better selection mechanism.

While formalizing the selection of an optimizer out of $A$ for optimizing a pipelines $p_{n^*_j}$ configuration as a multi-armed bandit problem in the form of the optimizer nodes $N^A_{n^*_j}$, it was utilized that most probably one of this optimizers $a_1$ will be the best suited one for the configuration optimization space of this particular pipeline.
This suitability of an optimizer for a space is dependent on the structure of the space, i.e. the number of dimensions as well as the value sets of the different dimension, and the properties and landscape of the target function in this space.\newline
But while $a_1$ is the best optimizer for the configuration space of this specific pipeline $p_{n^*_j}$, another optimizer $a_2$ could be the generally best optimizer for any pipeline that includes one certain component.
In that case, all sub-trees below nodes where this component was selected would ideally use $a_2$ as the optimization algorithm.
Because such hierarchical dependencies influence the best optimizer choice, the exploration and exploitation of a multi-armed bandit algorithm should utilized this dependencies.

The aforementioned upper confidence bounds were successively applied to the possibly hierarchical dependencies of trees in the form of the \textit{UCB applied to trees} algorithm (UCT)~\cite{Kocsis-UCT}, which is often also referred to as \textit{Monte-Carlo tree search} (MCTS).
Each node has an attached score, which is $ $ in the case of UCT.
MCTS is usually defined as an algorithm with an iteration of four steps, which are listed in the following and additionally illustrated in figure~\ref{fig:appraoch:mcts}:
\begin{enumerate}
    \item \textbf{Selection}: The next node for expansion is chosen.
    Commonly, each node has a value assigned and starting at the root node, the next node is the child node with the highest value until a not expanded node is reached.
    \item \textbf{Expansion}: The node is expanded, i.e. each possible follow-up state of the state in the unexpanded node is constructed and a child node is attached for each.
    \item \textbf{Simulation}: The new child nodes need some initial scores.
    Since an inner nodes represent an incomplete state, they are hard to score directly and a heuristic scoring for the sub-tree below this inner node is necessary.
    A set amount of Monte-Carlo Simulations, which are basically random walks down the search tree until a leaf node is reached, are performed.
    The scores of the states in the found leaf nodes are used to approximate a score this sub-tree.
    Thus, the scoring gets more accurate with a higher number of simulations per child node, since more different leaf nodes are reached and therefore more possible solution states are covered.
    But with a higher number of simulations, a bigger portion of the optimization budget is spent each iteration and fewer iterations can be performed overall.
    A careful trade-off is therefore necessary.
    \item \textbf{Backpropagation}: The simulations returned for child node $n$ the scores $x_1, ..., x_k$.
    Based on this scores $n$ will receive an initial scoring and afterwards, every node in the path from the root to $n$ will be updated to incorporate the newly gathered knowledge as well via backpropagation starting at $n$.
    At first, $n$ is scored initially with the following formula $\frac{\frac{\sum_{i=1}^k x_i}{k}}{v_r} + f \cdot \sqrt{\frac{\mathrm{ln} (V_r)}{v_r}}$.
    The formula consists of the following parts: $\frac{\sum_{i=1}^k x_i}{k}$ is the average of the simulation results, $f$ is a pre-defined weighting for the focus on exploration, $v_r$ is the amount of visits $n$ had after $r$ rounds of the MCTS loop via Monte-Carlo simulations before it was expanded or via previous backpropagation after it was expanded which is initially 1 since the node was just expanded and is receiving its first scoring, and $V_r$ is the $v_r$ value of the parent of $n$.
    For the actual backpropagation, starting with the parent of $n$ which is referred to as $n'$ and continuing until the root is reached each nodes score is updated similar to the scoring of $n$.
    At first, the $v_r$ counter of $n'$ is increased.
    Let $w_{r-1}$ be the scoring of $n'$ of the previous round, it will be updated during the backpropagation of this round with a cumulative moving average $w_r=w_{r-1} + \frac{w_c - w_{r-1}}{r}$, where $w_c$ is the current node score of the child node previously updated during the backpropagation.
    Now, the new score of $n'$ is $\frac{w_r}{v_r} + f \cdot \sqrt{\frac{\mathrm{ln} (V_r)}{v_r}}$.
\end{enumerate}
\begin{figure}[ht!]
    \centering
    \includegraphics[width=\textwidth]{gfx/Figures/Approach/MCTS.pdf}
    \caption{Illustration of the four MCTS steps. Inner graph nodes are shown in blue and the leaf nodes that are found during Monte-Carlo simulations are shown in red.}
    \label{fig:appraoch:mcts}
\end{figure}

To apply MCTS for a model selection in a search tree for example constructed with HTN planning, minor adjustments are necessary.
After incorporating the following changes, this modified MCTS is the algorithmic foundation of the approach of this thesis:
\begin{itemize}
    \item Optimizer nodes are leafs and can not be further expanded. If a Monte-Carlo simulation terminates at one or if the overall search tree is expanded to such an extend that an optimizer node is chosen in the Selection phase, a mode configuration run with a certain optimization budget is started with the algorithm represented by this node.
    \item The main MCTS loop does not run forever or for a fix amount of steps. It keeps track of the spent optimization budget during the search itself and during the optimizer runs and will stop after an iteration, if the budget limit is reached.
\end{itemize}
As inputs an implementation would require  some values:
\begin{itemize}
    \item The overall optimization budget $B$.
    \item The budget a single optimizer run for a MCTS simulation can spent $b \ll B$.
    \item The exploration factor $f$.
    \item The amount of Monte-Carlo simulation starting at each new child node after expansion.
\end{itemize}
Additionally, the definition of the HTN planning space and the set of optimizers $A$ are required to construct the search tree.
A complete formulation of the modified MCTS for the approach of this thesis can be found in appendix~\ref{sec:appendix:pseudo-code}.

\section{Model Configuration with Multiple Optimizers}
\label{sec:approach:configuration}
With the modified MCTS, the model selection is conducted and additionally it is controlled when an optimizer is started for model configuration.
But since the optimizers of the optimizer nodes $N^A_{n^*_j}$ below the pipeline node $n^*_j$, all try to optimize a configuration for pipeline $p_{n^*_j}$, they can utilize the fact they share the same optimization space.
At first, it is explained in the following how previous candidate evaluations of the shared optimization space are stored and afterwards, how optimizers re-use this stored candidate scores for new optimization runs, which is often also referred to as warmstarting.

\subsection{Shared Parameter Domain for Selected Models}
\label{sec:appraoch:configuration:parameter}
In every pipeline node $n^* \in N^*$, the optimization space for the model configuration of pipeline $p_{n^*}$ can be deduced.
Therefore, $n^*$ can be the single point of contact for every subjacent optimizer node $n^{a}_{n^*} \in N^A_{n^*}$.
With this centralized control instance it is possible to store and distribute knowledge from and to the optimizers.

At first, every $n^{a}_{n^*} \in N^A_{n^*}$ can get the concrete model configuration space from $n^*$, as well as other helpful information like for example the default parametrization of certain pipeline components, which may be a good starting point for some optimization algorithms to start their optimization process at.
Additionally, each optimizer can store their evaluated candidates together with the evaluation score at $n^*$.
Although this requires additional storage, collecting and saving already evaluated candidates has the following advantages if a suitable datastructure is used, which outweigh the disadvantage of higher storage demands since storage is in general a comparably cheap hardware component:
\begin{itemize}
    \item If saved in a hashtable, candidates can be inserted with a generated key and retrieved with one as well. Therewith, it can be checked if a candidate was already evaluated before and if yes, return the existing score from $n^*$ to the optimizer instead of training and evaluating a pipeline on the dataset again.
    The lookup complexity of a hashtable is constant, while training a machine learning model is very time consuming in comparison.
    For example, is the complexity of training a SVM with SMO~\cite{Platt-SMO} between linear and quadratic, depending on the dataset.
    \item If saved in a heap, retrieving the best candidates has a logarithmic complexity and thus, the best $n$ candidates from previous runs of the same or another optimizer from $N^A_{n^*}$ can efficiently be requested from $n^*$ as a starting point for a new optimization run.
\end{itemize}

\subsection{Warmstarting Versions of Optimization Algorithms}
\label{sec:appraoch:configuration:warmstart}
If the optimization budget is high enough, there is a high probability that the MCTS will select a pipeline node more than once or alternatively the Monte-Carlo simulations will end in the pipeline nodes again.
Additionally, all candidate evaluation are stored in these pipeline nodes for the given model configuration optimization space.
Hence, the optimization algorithms should utilize this advantage of prior collected data instead of performing the optimization anew every time.\newline
Starting with a foundation of previously collected data instead of a completely unexplored optimization space ist called \textit{warmstarting} an optimizer.
However, it is important to have a certain robustness against local optima, because without it an optimization algorithm could start in an local optimum, which was found in another optimization run, and would probably not get out of this local optima.
In the following it is explained how some of the different black-box optimization algorithms, which are presented in~\ref{sec:theory:optimization:search} up to~\ref{sec:theory:optimization:bayesian}, can be modified to be executed warmstarted.
They will therefore utilize an ordered list of pairs of the best candidates $l = \langle (c_1, s_1), ... (c_k, s_k) \rangle$, where $c_i$ is an already evaluated pipeline configuration and $s_i$ the corresponding score of this evaluation.

\textbf{Random Search}:
The first variant of a random search, where each candidate was drawn independent and at random, has no advantages from warmstarting besides not having to re-evaluate candidates that were already evaluated.\newline
But the second variant of a random search, where a candidate is sampled out of the hypersphere around the current best candidate up until either the optimization budget is spent or no improvement happened for $i$ iterations, can profit from a modification to a warmstarted version.
This version could use the top pair, or alternatively one randomly chosen pair of the top $n$ pairs for more variation, as a starting point.
Now, the random search will start sampling candidates in the hypersphere around this starting point and has the chance to find a better neighboring configuration.\newline
Of course the chosen starting point could be a local optimum, but this random search variant will use this hypersphere for at most $i$ iterations if no improvement is achieved, which can be an indicator for an optimum.
In this case, it will select a new random starting point.
Thus, not more than $i$ iterations are lost to a local optimum selected from $l$, which is depending on the optimization budget usually manageable, since random search is a comparably simplistic and therefore fast optimization algorithm.

\textbf{Hyperband}:
Hyperband is basically a method of treating multiple, parallel running random searches as a multi-armed bandit problem and stopping the less successful instances to have more optmization budget for the better performing instances.
Therewith, if each random search instance is implemented as the warmstarted version from the previous explanation, Hyperband can profit from warmstarting a well.\newline
The typical multi-armed bandit problem of balancing exploration and exploitation should be applied here as well.
If Hyperband is configured to have $k$ parallel running instances, $\lfloor \sigma \cdot k \rfloor$ instances with $\sigma \in (0,1)$ should start at one of the top candidates from $l$ for exploitation, while $\lceil (1 - \sigma) \cdot k \rceil$ instances should start at new and randomly chosen configurations for a better exploration coverage of each Hyperband run.

\textbf{Genetic Optimization}:
A very important part of a genetic optimization algorithm, is the selection scheme from which individuals the next generation will be constructed.
Such a selection can vary from simple approaches, like for example a random selection or the best individuals from the current generation, up to more sophisticated schemes like for example a \textit{Boltzmann Tournament Selection}~\cite{Goldberg-Boltzmann}.\newline
With such an selection scheme in place, the warmstarting can be implemented straightforward.
Instead of selecting individuals be a selection scheme from a previous generation, the selection scheme of the genetic algorithm will be applied on a subset of $l$.

\textbf{SMAC}:
For the Bayesian optimization implementation \textit{SMAC}, a warmstarting variant was developed by~\textcite{Lindauer-Smac-Warmstart}.
During the optimization, SMAC trains a regression model, for instance a Random Forest, to predict the performance of a configuration before actually evaluating it.
This regression model is then used to guide the selection of candidates that are actually evaluated to build the surrogate function model.
Additionally to training this regression model progressively during the optimization, it can be trained beforehand with a subset of $l$ to have a more accurate model right from the start.
