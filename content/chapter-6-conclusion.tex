% !TEX root = ../my-thesis.tex
%
\chapter{Conclusion}
\label{sec:conclusion}
In this final chapter after previously answering the concrete research questions of the thesis, the prior descriptions and findings regarding the optimizer ensemble AutoML approach are summarized and concluded with an outlook of possible future works.

\section{Summary and Findings}
\label{sec:conclusion:summary}
This thesis was based on the statements of the No-Free-Lunch theorems for optimization, which had proven that under given constraints no optimization algorithm can be superior for all problems.
To approach this problem, the model selection of an AutoML setting combined with choosing an optimizer that should perform a model configuration for the selected pipeline model was seen as a hierarchical Multi-Armed Bandit problem.
Hence, it was tackled with a MCTS, which searches a model selection graph that is realized as a HTN planning graph with embedded optimizers.
This optimizers are combined as an ensemble to utilize each others gathered information about evaluated candidates and perform successive warmstarting for each optimization run.

This novelle AutoML idea was realized as a first simple reference implementation, which afterwards was used in a series of experiments to examine the feasibility of the approach and to gather additional knowledge about the functionality of the components.

In its current reference implementation the approach of this thesis was not able to surpass the currently best state-of-the-art approaches auto-sklearn and TPOT for the examined datasets.
However, it was able to show competitive results for some of the datasets which could indicate the potential to close the current gap between the tools or even surpass them.

\section{Open Questions and Future Work}
Besides the mentioned further development regarding general implementation maturity and optimization, some other factors, which could lead to better results, were not covered by this thesis and are good starting points for future work.
To conclude this chapter, they are listed in the following, grouped into different categories:
\begin{itemize}
    \item \textbf{Configuration}: The evaluation was solely done with one configuration of this approach.
    Although the utilized configuration showed the best results in some tests prior to the experiments, there may exist even better configuration, for example regarding the amount of Monte-Carlo simulations or the time budget of each single optimization run.
    A thorough study of a variety of possible configurations in the context of different AutoML timeouts and more datasets than the evaluated ten, can be a first next step to identify better configurations.\newline
    Additionally, the configuration of this approach remained static during the entire MCTS search.
    Modifying the configuration values during the search, could improve the balance between exploration and exploitation.
    For example, the amount of Monte-Carlo simulation could start with a high value and will be reduced during the run, such that a higher exploration coverage of the search space is reached in the beginning.
    Or alternatively the optimization time budget could start with a low value and be improved later in the search, once the probability of exploiting more suitable optimizers is higher after a certain degree of exploration.
    The adjustment of time budgets, could utilize one of the time allocation policies of~\textcite{Quemy-Two-Stage-Optimization}, which were presented in~\ref{sec:theory:related}.
    \item \textbf{Model Selection Search}: During the evaluation of different configurations, it could also prove worthwhile to evaluate different search algorithms instead of a MCTS.
    Since the variant of this approach that used a random search instead of MCTS was not significantly worse, other search algorithms should also be considered and tested.\newline
    Although MCTS is usually associated with UCT as a policy for scoring nodes, there are also other published MCTS policies for scoring nodes and selecting them accordingly.
    A survey of a broad selection of such policies was for example done by~\textcite{Browne-Policies}.
    Besides completely different search algorithms, a study of different MCTS variations and policies could also be worth doing.
    \item \textbf{Optimizer Ensemble Model Configuration}: In the case of the evaluated datasets, MCTS rarely exploited a random search or discretization search for optimization in comparison to the other optimizers.
    If this is also the case during a broader study of more datasets, the subset of optimizers that are selected for the ensemble, could be reduced to the remaining three optimizers, or alternatively the two optimizers could be replaced by other optimizers, which were not integrated and evaluated yet.\newline
    Besides the selection of optimizers for the ensembles, the interaction within the ensembles is also a starting point for future research.
    It has to be evaluated if every optimizer really profits from warmstarting or if this leads for example to an over-searching of local optima.\newline
    Similar to adjusting the configuration dynamically during the run, it could also be a variation to enable or disable warmstarting for different time periods of the overall search or for different parts of the search graph.
    In the same manner, early stopping for all or for some optimizers could be added as a functionality and then dynamically be enabled and disabled during the run, if there is a chance for a better budget allocation towards more promising optimizers.\newline
    Additionally, the possibilities for creating the model configuration optimization spaces can be extended to a degree of the expressiveness of auto-sklearns space modelling features.
    Constraints across multiple parameters like for example, "\textit{If value $a$ is selected for parameter $p_1$, value $b$ is no longer an allowed value for parameter $p_2$}" could prevent this approach from evaluating unsuitable or even invalid pipeline parametrization candidates.
    \item \textbf{Problem Settings}: In theory, the approach of this thesis has no limitations regarding the desired functionality of the algorithm selection and hyperparameter optimization outcome.
    This thesis focusses on AutoML as one specific outcome, but with minor modifications regarding problem description input and internal candidate evaluation, it can construct models for other algorithm configurations domains as well.\newline
    Once the above mentioned improvements were made and this minor modifications for allowing other domains as well, an interesting follow-up research question is to evaluate how the optimizer ensemble approach performs in other domains besides AutoML.
\end{itemize}
