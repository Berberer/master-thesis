% !TEX root = ../my-thesis.tex
%
\chapter{Empirical Analysis and Evaluation}
\label{sec:evaluation}
After devising this approach for optimizer ensembles and implementing it as a reference implementation, it is essential to evaluate and asses this approach.
This assessment is the goal of this chapter and the final step of this thesis before summing up and considering the outcomes of it.
The evaluated reference implementation is nicknamed \textit{frankensteins-automl} and will henceforth be referenced by this name, or alternatively with the shorter version \textit{frankenstein}, while describing the experiment setup as well as listing and evaluating the experiment results.

To have a clear guided structure for this assessment of frankensteins-automl, it is based on the research questions presented in~\ref{sec:intro:goal}, which is elucidated here in more detail at first with a focus on which data is required to answer them.
Afterwards, this chapter answers the research questions with a series of Empirical evaluations in the form of experiments.\newline
Therefore, the setup and configuration of the experiments is given as a starting point to subsequently analyze the results of the experiments and to deduce the answers to the research questions from them.

\section{Research Questions in Detail}
The first research question was about the feasibility of the approach, i.e. is it a viable idea to optimize an AutoML problem solution with an optimizer ensemble.
This will be answered in two steps.\newline
At first, are the solution scores of this approach similar or better than the scores of other state-of-the-art approaches for several different datasets?
If they are not at all or not for a high number of datasets, frankensteins-automl is not a competitor to the state-of-the-art approaches.
In this case, there would be no progress in this scientific field based on this approach and no motivation to utilize it further without advances being made regarding this approach in future work.\newline
Secondly, if there is an improvement in some scores, which component of the approach, the MCTS model selection and the warmstarted optimizer ensemble, has which influence on the improvements.
To assess this influences, three versions of frankensteins-automl are evaluated, where two of them have slight modifications:
\begin{enumerate}
    \item To evaluate the influence of a Model Selection via a MCTS, this modified version does not utilize a optimizer ensemble.
    Every optimizer node represents the same optimizer, which will be SMAC for this evaluation.
    In the remaining chapter it will be referenced as \textit{frankenstein-mcts.}
    \item To evaluate the influence of the ensembled optimizers, this modified version does not utilize a MCTS selection.
    The node selection on each level down the paths to the different optimizer nodes, will be random for each search iteration.
    Therefore, it is basically a repeated random search.
    Hence, it will be named \textit{frankenstein-rs}.
    \item Both components are included in this version, i.e. the original and unmodified \textit{frankenstein} is evaluated as described in the two previous chapters.
\end{enumerate}
By comparing the scores of the tree variants for different datasets and different timeouts, data about the scores with and without one component can be gathered, which can be used to analyze the influence of the component.

As the second research question, the focus is on the utilization of the different optimizers.
During each run of frankensteins-automl, not for the variants with the modifications, it will be counted how many times each optimizer was selected by the MCTS and therewith an optimization run with this optimizer started.
Since the MCTS tries to find the best suited optimizer and to exploit it, this selection frequency is an indicator for the suitability of this optimizer to the input dataset.\newline
The frequencies will be aggregated together with the AutoML time budget and the input dataset.
Thus, it is possible to check if one or more optimizers were preferably selected in general for all settings or just in an AutoML problem setting with a certain time budget or a input dataset with certain properties.
Alternatively, there could be no significant difference and each optimizer was selected comparably often across all observed AutoML settings.

\section{Experiment Setup}
\label{sec:evaluation:setup}
To answer the first step of the first research question, frankensteins-automl will be evaluated against different other competitor state-of-the-art approaches.
The selection of this approaches are chosen to represent the difference optimization strategies.
\textit{ML-Plan} for optimization via searching, \textit{TPOT} as a variant with a Genetic Algorithm, \textit{auto-sklearn} for Bayesian optimization, and finally \textit{Mosaic} as a related work approach that utilizes more than one optimization method via an optimization space transformation.\newline
Since ML-Plan is the only approach out of this selection, which is not implemented in Python but in Java, it can not be integrated seamlessly in the same experimental setup as the remaining ones.
The experimental results for ML-Plan were kindly provided by the original authors of the approach~\textcite{Mohr-ML-Plan}.
All other approaches will be evaluated in a unified Python experimental environment\footnote{\url{https://github.com/Berberer/python-experimenter}}.

The experimental setup has a certain set of constraints and rules, which are applied to all approaches to achieve a fair and meaningful comparison
They are summarized in the following:
\begin{enumerate}
    \item Each AutoML experiment setting (i.e. a combination of dataset + timeout + approach) will be conducted with 30 different random seeds to achieve a better statistical robustness against outliers in the evaluation scores.
    \item The different approaches have be 60 Gb RAM and 16 CPU cores, Intel Xeon E5-2670 with 2.6 Ghz, as a hardware limitation.
    \item A pipeline candidate evaluation during the AutoML process, i.e. constructing, training and testing a candidate, does not take longer than five minutes and will be terminated otherwise.
    \item All AutoML approaches will utilize the model selection and configuration space from auto-sklearn, to prevent that one approach appears superior because it utilizes one pipeline component to which the other approaches have no access in their model selection phase.
    \item Before the AutoML approaches are started, the dataset is separated into two stratified splits. The AutoML approach gets the 70\% split as an input and the resulting pipeline of the approach will be evaluated with the other 30\% split to get a final score in the form of an accuracy measurement.
\end{enumerate}
The approaches are configured to comply with this constraints, but all configuration possibilities of the corresponding frameworks besides that are kept as their default configuration value for the experiments.\newline
Restricting the optimization spaces to the auto-sklearn space means having at most pipelines of length three (data pre-processing, feature pre-processing, classification model/ensemble) as well as determining which components are allowed and which parameters of this components are included in the model configuration space.
Eventually, this nay limit the capabilities of frankensteins-automl, as well as ML-Plan and TPOT, because they are capable of constructing more complex pipelines.
But this limitations are important for an informative and meaningful evaluation.\newline
In this manner, the actual accomplishments of the approaches can be evaluated and compared to each other under fair conditions.
It can be excluded that one approach outperforms another because his model selection space contains for example one component that the other approaches does not have for selection.\newline
The adaptation of the auto-sklearn space in the JSON format of this approach, which is used by frankensteins-automl for the experiments, can be found in appendix~\ref{sec:appendix:htn-space} as well as a description of the limits this adaptation has in comparison to the original auto-sklearn search space.

In a first step of answering the first research question, the 4 state-of-the-art approaches as well as frankenstein have a timeout of one hour for selecting and configuring a pipeline.
For the dataset inputs in this first experiments, ten different datasets from the \textit{OpenML} platform~\cite{Vanschoren-OpenML} were selected:
\begin{itemize}
    \item AMAZON\footnote{\url{https://www.openml.org/d/1457}}
    \item CAR\footnote{\url{https://www.openml.org/d/40975}}
    \item CIFAR10SMALL\footnote{\url{https://www.openml.org/d/40926}}
    \item DEXTER\footnote{\url{https://www.openml.org/d/4136}}
    \item DOROTHEA\footnote{\url{https://www.openml.org/d/4137}}
    \item KRVSKP\footnote{\url{https://www.openml.org/d/3}}
    \item SEMEION\footnote{\url{https://www.openml.org/d/1501}}
    \item WAVEFORM\footnote{\url{https://www.openml.org/d/60}}
    \item WINEQUALITY\footnote{\url{https://www.openml.org/d/40498}}
    \item YEAST\footnote{\url{https://www.openml.org/d/181}}
\end{itemize}

The second step, i.e. the comparison of the influence of the different components of this approach via the three modifications, has a similar setup.
Again, the five experiment constraints are applied and the same ten datasets are given as inputs.
But to evaluate if the different influences might change with a different timescale, this second experiment will have one hour, six hours and twelve hours as timeouts for the three variants frankenstein, frankenstein-rs and frankenstein-mcts.

During the two experiment stages, frankensteins-automl (in the unmodified version for the second stage) will additionally count the frequencies of utilizing each type of optimizer.
With this counting, there is aggregated data of the optimizer utilization for three different timeouts and ten different datasets.
The properties of the ten datasets, for example number of classes or datapoints, balance of datapoints for each class, or types of attributes, will be evaluated in the context of the optimizer utilization frequencies to try to extrapolate knowledge about the suitability of the different optimizers for different dataset types to answer the second research question.

For both experiment stages, frankensteins-automl is configured with the following:
\begin{itemize}
    \item When a node is expanded during the MCTS, each new child node is scored with the results of three Monte-Carlo simulations.
    \item Each run of one of the optimizers in their leaf nodes has an optimization time budget of three minutes.
\end{itemize}
These values were chosen after some initial experiments to balance the hardware workload accordingly to the experiment hardware and the solution quality.
They may not be the best configuration values for every use-case and every input dataset.
A more thorough survey of possible configurations for this approach can be a starting point for further research.

For a better reproducibility of this experiments, they are conducted inside of a \textit{Singularity} container~\cite{Kurtzer-Singularity}.
Hence, the results can be reproduced in every environment where Singularity is installed to verify the data or to conduct customized variants of these experiments.
The Singularity image recipe as well as some additional information regarding the setup can be found in appendix~\ref{sec:appendix:singularity}.

\section{Results of the Experiments}
\label{sec:evaluation:results}

\Blindtext

\section{Analysis of the Experiment Outcomes}
\label{sec:evaluation:analysis}

\Blindtext
