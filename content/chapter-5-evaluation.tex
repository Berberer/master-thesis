% !TEX root = ../my-thesis.tex
%
\chapter{Empirical Analysis and Evaluation}
\label{sec:evaluation}
After devising this approach for optimizer ensembles and implementing it as a reference implementation, it is essential to evaluate and asses this approach.
This assessment is the goal of this chapter and the final step of this thesis before summing up and considering the outcomes of it.
The evaluated reference implementation is nicknamed \textit{frankensteins-automl}, because this AutoML approach is metaphorically stitched together from different optimizers like Frankenstein's Monster.
It will henceforth be referenced by this name, or alternatively with the shorter version \textit{frankenstein}, while describing the experiment setup as well as listing and evaluating the experiment results.

To have a clear guided structure for this assessment of frankensteins-automl, it is based on the research questions presented in~\ref{sec:intro:goal}, which is elucidated here in more detail at first with a focus on which data is required to answer them.
Afterwards, this chapter answers the research questions with a series of Empirical evaluations in the form of experiments.\newline
Therefore, the setup and configuration of the experiments is given as a starting point to subsequently analyze the results of the experiments and to deduce the answers to the research questions from them.

\section{Research Questions in Detail}
The first research question was about the feasibility of the approach, i.e. is it a viable idea to optimize an AutoML problem solution with an optimizer ensemble.
This will be answered in two steps.\newline
At first, are the solution scores of this approach similar or better than the scores of other state-of-the-art approaches for several different datasets?
If they are not at all or not for a high number of datasets, frankensteins-automl is not a competitor to the state-of-the-art approaches.
In this case, there would be no progress in this scientific field based on this approach and no motivation to utilize it further without advances being made regarding this approach in future work.\newline
Secondly, if there is an improvement in some scores, which component of the approach, the MCTS model selection and the warmstarted optimizer ensemble, has which influence on the improvements.
To assess this influences, three versions of frankensteins-automl are evaluated, where two of them have slight modifications:
\begin{enumerate}
    \item To evaluate the influence of a Model Selection via a MCTS, this modified version does not utilize a optimizer ensemble.
    Every optimizer node represents the same optimizer, which will be SMAC for this evaluation.
    In the remaining chapter it will be referenced as \textit{frankenstein-mcts.}
    \item To evaluate the influence of the ensembled optimizers, this modified version does not utilize a MCTS selection.
    The node selection on each level down the paths to the different optimizer nodes, will be random for each search iteration.
    Therefore, it is basically a repeated random search.
    Hence, it will be named \textit{frankenstein-rs}.
    \item Both components are included in this version, i.e. the original and unmodified \textit{frankenstein} is evaluated as described in the two previous chapters.
\end{enumerate}
By comparing the scores of the tree variants for different datasets and different timeouts, data about the scores with and without one component can be gathered, which can be used to analyze the influence of the component.

As the second research question, the focus is on the utilization of the different optimizers.
During each run of frankensteins-automl, not for the variants with the modifications, it will be counted how many times each optimizer was selected by the MCTS and therewith an optimization run with this optimizer started.
Since the MCTS tries to find the best suited optimizer and to exploit it, this selection frequency is an indicator for the suitability of this optimizer to the input dataset.\newline
The frequencies will be aggregated together with properties of the AutoML input dataset.
Thus, it is possible to check if one or more optimizers were preferably selected in general for all settings, i.e. for all datasets, or just in an AutoML problem setting with a certain input dataset that has certain properties.
Alternatively, there could be no significant difference and each optimizer was selected comparably often across all observed AutoML settings or one or more optimizer were used primarily unrelated to the dataset.

\section{Experiment Setup}
\label{sec:evaluation:setup}
To answer the first step of the first research question, frankensteins-automl will be evaluated against different other competitor state-of-the-art approaches.
The selection of this approaches are chosen to represent the difference optimization strategies.
\textit{ML-Plan} for optimization via searching, \textit{TPOT} as a variant with a Genetic Algorithm, \textit{auto-sklearn} for Bayesian optimization, and finally \textit{Mosaic} as a related work approach that utilizes more than one optimization method via an optimization space transformation.\newline
Since ML-Plan is the only approach out of this selection, which is not implemented in Python but in Java, it can not be integrated seamlessly in the same experimental setup as the remaining ones.
The experimental results for ML-Plan were kindly provided by the original authors of the approach~\textcite{Mohr-ML-Plan}.
All other approaches will be evaluated in a unified Python experimental environment\footnote{\url{https://github.com/Berberer/python-experimenter}}.

The experimental setup has a certain set of constraints and rules, which are applied to all approaches to achieve a fair and meaningful comparison
They are summarized in the following:
\begin{enumerate}
    \item Each AutoML experiment setting (i.e. a combination of dataset + timeout + approach) will be conducted with 30 different random seeds to achieve a better statistical robustness against outliers in the evaluation scores.
    \item The different approaches have be 60 Gb RAM and 16 CPU cores, Intel Xeon E5-2670 with 2.6 Ghz, as a hardware limitation.
    \item A pipeline candidate evaluation during the AutoML process, i.e. constructing, training and testing a candidate, does not take longer than five minutes and will be terminated otherwise.
    \item All AutoML approaches will utilize the model selection and configuration space from auto-sklearn, to prevent that one approach appears superior because it utilizes one pipeline component to which the other approaches have no access in their model selection phase.
    \item Before the AutoML approaches are started, the dataset is separated into two stratified splits. The AutoML approach gets the 70\% split as an input and the resulting pipeline of the approach will be evaluated with the other 30\% split to get a final score in the form of an accuracy measurement.
\end{enumerate}
The approaches are configured to comply with this constraints, but all configuration possibilities of the corresponding frameworks besides that are kept as their default configuration value for the experiments.\newline
Restricting the optimization spaces to the auto-sklearn space means having at most pipelines of length three (data pre-processing, feature pre-processing, classification model/ensemble) as well as determining which components are allowed and which parameters of this components are included in the model configuration space.
Eventually, this nay limit the capabilities of frankensteins-automl, as well as ML-Plan and TPOT, because they are capable of constructing more complex pipelines.
But this limitations are important for an informative and meaningful evaluation.\newline
In this manner, the actual accomplishments of the approaches can be evaluated and compared to each other under fair conditions.
It can be excluded that one approach outperforms another because his model selection space contains for example one component that the other approaches does not have for selection.\newline
The adaptation of the auto-sklearn space in the JSON format of this approach, which is used by frankensteins-automl for the experiments, can be found in appendix~\ref{sec:appendix:htn-space} as well as a description of the limits this adaptation has in comparison to the original auto-sklearn search space.

In a first step of answering the first research question, the 4 state-of-the-art approaches as well as frankenstein have a timeout of one hour for selecting and configuring a pipeline.
For the dataset inputs in this first experiments, ten different datasets from the \textit{OpenML} platform~\cite{Vanschoren-OpenML} were selected:
\begin{itemize}
    \item AMAZON\footnote{\url{https://www.openml.org/d/1457}}
    \item CAR\footnote{\url{https://www.openml.org/d/40975}}
    \item CIFAR10SMALL\footnote{\url{https://www.openml.org/d/40926}}
    \item DEXTER\footnote{\url{https://www.openml.org/d/4136}}
    \item DOROTHEA\footnote{\url{https://www.openml.org/d/4137}}
    \item KRVSKP\footnote{\url{https://www.openml.org/d/3}}
    \item SEMEION\footnote{\url{https://www.openml.org/d/1501}}
    \item WAVEFORM\footnote{\url{https://www.openml.org/d/60}}
    \item WINEQUALITY\footnote{\url{https://www.openml.org/d/40498}}
    \item YEAST\footnote{\url{https://www.openml.org/d/181}}
\end{itemize}

The second step, i.e. the comparison of the influence of the different components of this approach via the three modifications, has a similar setup.
Again, the five experiment constraints are applied and the same ten datasets are given as inputs.
But to evaluate if the different influences might change with a different timescale, this second experiment will have one hour, six hours and twelve hours as timeouts for the three variants frankenstein, frankenstein-rs and frankenstein-mcts.

During the two experiment stages, frankensteins-automl (in the unmodified version for the second stage) will additionally count the frequencies of utilizing each type of optimizer.
With this counting, there is aggregated data of the optimizer utilization for three different timeouts and ten different datasets.
The properties of the ten datasets, for example number of classes or datapoints, balance of datapoints for each class, or types of attributes, will be evaluated in the context of the optimizer utilization frequencies to try to extrapolate knowledge about the suitability of the different optimizers for different dataset types to answer the second research question.

For both experiment stages, frankensteins-automl is configured with the following:
\begin{itemize}
    \item When a node is expanded during the MCTS, each new child node is scored with the results of three Monte-Carlo simulations.
    \item Each run of one of the optimizers in their leaf nodes has an optimization time budget of three minutes.
\end{itemize}
These values were chosen after some initial experiments to balance the hardware workload accordingly to the experiment hardware and the solution quality.
They may not be the best configuration values for every use-case and every input dataset.
A more thorough survey of possible configurations for this approach can be a starting point for further research.

For a better reproducibility of this experiments, they are conducted inside of a \textit{Singularity} container~\cite{Kurtzer-Singularity}.
Hence, the results can be reproduced in every environment where Singularity is installed to verify the data or to conduct customized variants of these experiments.
The Singularity image recipe as well as some additional information regarding the setup can be found in appendix~\ref{sec:appendix:singularity}.

\section{Results of the Experiments}
\label{sec:evaluation:results}
 The results of these experiments are presented and assessed in the following, before they are evaluated in more detail and used to answer the research questions in the following chapter.

 At first, table~\ref{table:benchmark-results} shows the benchmark results of frankensteins-automl against the state-of the art approaches.
 The experiment runs were only classified as successful and used in the results, if the AutoML approach was able to return a valid pipeline in one hour plus an additional 15 minutes range of tolerance and without throwing an exception or error.
 None of the approaches was able to achieve successful runs for all 30 seeds across all ten datasets.
 But especially Mosaic was very unstable and was not able to achieve at least results for half of the seeds for six out of the ten datasets.
 Hence, the significance of the comparison to Mosaic is very problematic.\newline
 Nevertheless, a check for significant improvement or deterioration in comparison to the scores of frankenstein were executed via a two-sample \textit{t}-tests with $p = 0.05$..
 The absolute frequencies how often the approaches were significantly better, significantly worse or without a significant difference out of the ten datasets are shown in table~\ref{table:significanse-counts}.

\begin{sidewaystable}[ht]
    \renewcommand{\arraystretch}{1.5}
    \centering
    \caption[Results of the benchmark experiments for comparing frankenstein with other state-of-the-art AutoML approaches.]{
        Results of the benchmark experiments for comparing frankenstein with other state-of-the-art AutoML approaches.
        The individual cells have the following format: $\text{average} \pm \text{standard deviation} (\text{amount of successful experiment runs})$.
        Additionally, there is a $\uparrow$ or $\downarrow$ if the score of the compared AutoML approach is significantly better or significantly worse compared to frankenstein.
        This statistical significance is tested via two-sample \textit{t}-tests with $p = 0.05$.
        If the AutoML approach had not a single successful experiments run for the corresponding dataset, the cell is filled with an $\mathbin{/}$.
    }
    \label{table:benchmark-results}
    \begin{tabular}{l|ccccc}
        & \texttt{frankenstein}  & \texttt{autosklearn}  & \texttt{mlplan}  & \texttt{mosaic}  & \texttt{tpot} \\
        \hline
        \texttt{amazon} & $ 0.9735 \pm 0.0049 (30) $ & $ 0.9752 \pm 0.0018 (29) \phantom{\downarrow}$ & $ 0.7301 \pm 0.0384 (20) \downarrow$ & $ \mathbin{/}   \phantom{\downarrow}$ & $ 0.9753 \pm 0.0024 (29) \phantom{\downarrow}$\\
        \texttt{car} & $ 0.9789 \pm 0.0426 (30) $ & $ 0.9874 \pm 0.0065 (26) \phantom{\downarrow}$ & $ 0.4430 \pm 0.2452 (19) \downarrow$ & $ 0.9777 \pm 0.0145 (09) \phantom{\downarrow}$ & $ 0.9900 \pm 0.0092 (30) \phantom{\downarrow}$\\
        \texttt{cifar} & $ 0.3077 \pm 0.0622 (25) $ & $ 0.3997 \pm 0.0068 (28) \uparrow$ & $ 0.4226 \pm 0.0080 (20) \uparrow$ & $ 0.3663 \pm 0.0086 (27) \uparrow$ & $ 0.2815 \pm 0.0404 (23) \phantom{\downarrow}$\\
        \texttt{dexter} & $ 0.9004 \pm 0.0453 (28) $ & $ 0.9263 \pm 0.0189 (30) \uparrow$ & $ 0.9447 \pm 0.0170 (20) \uparrow$ & $ 0.9509 \pm 0.0165 (22) \uparrow$ & $ 0.9289 \pm 0.0231 (30) \uparrow$\\
        \texttt{dorothea} & $ 0.9229 \pm 0.0130 (27) $ & $ 0.9332 \pm 0.0085 (30) \uparrow$ & $ \mathbin{/}   \phantom{\downarrow}$ & $ 0.9469 \pm 0.0124 (28) \uparrow$ & $ 0.9275 \pm 0.0117 (26) \phantom{\downarrow}$\\
        \texttt{krvskp} & $ 0.9933 \pm 0.0023 (27) $ & $ 0.9925 \pm 0.0034 (25) \phantom{\downarrow}$ & $ 0.5017 \pm 0.0171 (19) \downarrow$ & $ 0.9932 \pm 0.0022 (03) \phantom{\downarrow}$ & $ 0.9934 \pm 0.0024 (30) \phantom{\downarrow}$\\
        \texttt{semeion} & $ 0.9235 \pm 0.0588 (30) $ & $ 0.9468 \pm 0.0125 (29) \uparrow$ & $ 0.1538 \pm 0.0375 (20) \downarrow$ & $ 0.9423 \pm 0.0102 (12) \phantom{\downarrow}$ & $ 0.9356 \pm 0.0129 (30) \phantom{\downarrow}$\\
        \texttt{waveform} & $ 0.8412 \pm 0.0278 (29) $ & $ 0.8588 \pm 0.0074 (29) \uparrow$ & $ 0.8645 \pm 0.0079 (17) \uparrow$ & $ 0.8686 \pm 0.0079 (05) \uparrow$ & $ 0.8606 \pm 0.0075 (30) \uparrow$\\
        \texttt{wine} & $ 0.5292 \pm 0.1629 (28) $ & $ 0.6329 \pm 0.0096 (29) \uparrow$ & $ 0.6601 \pm 0.0123 (20) \uparrow$ & $ 0.6446 \pm 0.0116 (17) \uparrow$ & $ 0.6614 \pm 0.0127 (30) \uparrow$\\
        \texttt{yeast} & $ 0.5752 \pm 0.0592 (30) $ & $ 0.6009 \pm 0.0169 (30) \uparrow$ & $ 0.2987 \pm 0.0623 (20) \downarrow$ & $ 0.6126 \pm 0.0044 (02) \phantom{\downarrow}$ & $ 0.6048 \pm 0.0180 (30) \uparrow$\\
        \hline
    \end{tabular}
\end{sidewaystable}

\begin{table}[ht]
    \renewcommand{\arraystretch}{1.5}
    \centering
    \caption[Absolute frequencies of significant improvements or deterioration.]{Absolute frequencies of significant improvements or deterioration of the different benchmark approaches compared to frankenstein for the different datasets. For the two cases, where the benchmark approach was not able to produce a single result, this was counted as \textit{Significantly worse} compared to frankenstein, because no valid result is basically a score of $0$.}
    \label{table:significanse-counts}
    \begin{tabular}{l|ccc}
        & \textit{Significantly better} & \textit{Significantly worse} & \textit{No significant difference} \\
        \hline
        \texttt{autosklearn} & $7$ & $0$ & $3$ \\
        \texttt{mlplan} & $4$ & $6$ & $0$ \\
        \texttt{mosaic} & $5$ & $1$ & $4$ \\
        \texttt{tpot} & $4$ & $0$ & $6$ \\
        \hline
    \end{tabular}
\end{table}

Additionally, the approach of this thesis was evaluated by itself in more detail.
The first evaluated aspect of frankensteins-automl was the usage of the different optimization algorithms of the ensemble.\newline
Table~\ref{table:optimizer-calls} show the relative frequency of the optimizers in the context of corresponding datasets as well a Visualization of this frequencies as a heatmap.
Here, the data was deliberately only considered relatively because the data was aggregated among all three different timeouts (1h, 6h, 12).
Therefore, absolute values are less meaningful because with different timeouts the amount of MCTS iterations and thus the amount of optimization runs differs greatly.

\begin{table}[ht]
    \caption{The relative frequency of optimizer calls for the different datasets.}
    \label{table:optimizer-calls}
    \begin{subtable}{\textwidth}
        \centering
        \caption[Numerical values with four decimal places.]{Numerical values with four decimal places. RS = Random Search, HB = Hyperband, GA = Genetic Algorithm, DS = Discretization Search}
        \renewcommand{\arraystretch}{1.5}
        \begin{tabular}{l|ccccc}
            & \textit{RS} & \textit{HB} & \textit{GA} & \textit{SMAC} & \textit{DS} \\
            \hline
            \texttt{amazon} & $0.0669$ & $0.1811$ & $0.2732$ & $0.3840$ & $0.0947$ \\
            \texttt{car} & $0.0679$ & $0.2185$ & $0.2194$ & $0.4202$ & $0.0740$ \\
            \texttt{cifar} & $0.1545$ & $0.1591$ & $0.1640$ & $0.3721$ & $0.1501$ \\
            \texttt{dexter} & $0.0780$ & $0.1052$ & $0.2991$ & $0.4396$ & $0.0781$ \\
            \texttt{dorothea} & $0.1470$ & $0.1502$ & $0.2415$ & $0.3153$ & $0.1460$ \\
            \texttt{krvskp} & $0.0717$ & $0.1835$ & $0.3036$ & $0.3666$ & $0.0746$ \\
            \texttt{semeion} & $0.0860$ & $0.1596$ & $0.2913$ & $0.3748$ & $0.0884$ \\
            \texttt{waveform} & $0.0708$ & $0.1291$ & $0.2918$ & $0.4294$ & $0.0789$ \\
            \texttt{wine} & $0.0692$ & $0.2622$ & $0.2524$ & $0.3419$ & $0.0742$ \\
            \texttt{yeast} & $0.0701$ & $0.3560$ & $0.1434$ & $0.3636$ & $0.0669$ \\
            \hline
        \end{tabular}
    \end{subtable}
    \par\bigskip
    \begin{subfigure}{\textwidth}
        \centering
        \caption{Visualization of the frequencies as a heatmap.}
        \includegraphics[width=\textwidth,keepaspectratio]{gfx/Figures/Evaluation/OptimizerCallsHeatmap.pdf}
    \end{subfigure}
\end{table}

After listing the relative optimizer utilization frequency, this optimizer utilization is considered in the context of the different datasets and their properties in table~\ref{table:optimizer-call-correlations}.
The following general dataset properties were gathered from OpenML for each dataset:
\begin{itemize}
    \item \textit{Instances}: The number of instances of the dataset.
    \item \textit{Features}: The number of features that each instance has.
    \item \textit{Classes}: The number of available target classes the instances are classified into.
    \item \textit{Dimensionality}: The relationship between features and instances that is calculated via \textit{Features} divided by \textit{Instances}.
    \item \textit{Autocorrelation}: After assigning consecutive numbers to the classes, the average difference of the classes successive instances.
    \item \textit{Minority Class}: How much percent of \textit{Instances} is classified by the minority class.
    \item \textit{Majority Class}: How much percent of \textit{Instances} is classified by the majority class.
    \item \textit{Class Entropy}: Information theoretic entropy value of the class values.
\end{itemize}
These values were examined in terms of a correlation with the optimizer utilization of the five different optimization algorithms.
For the sake of completeness, this was here not only done for the relative frequency but also for the absolute frequency and for the the rank of the five optimizer utilization values as well.\newline
Because linearity and an underlying normal distribution cannot be ensured, since the datasets were deliberately chosen to be very different from one another, the correlations with relative and absolute frequency as well as rank have to be calculated with the non-parametric Spearman's rank correlation coefficient $\rho$.

\begin{table}[ht]
    \caption[Analysis of possible correlations between optimizer utilization and dataset properties.]{Analysis of possible correlations between optimizer utilization and dataset properties via the Spearman's rank correlation coefficient $\rho$. RS = Random Search, HB = Hyperband, GA = Genetic Algorithm, DS = Discretization Search}
    \label{table:optimizer-call-correlations}
    \renewcommand{\arraystretch}{1.25}
    \begin{subtable}{\textwidth}
        \centering
        \caption{Relative utilization frequency.}
        \begin{tabular}{l|ccccc}
            & \textit{RS} & \textit{HB} & \textit{GA} & \textit{SMAC} & \textit{DS} \\
            \hline
            \textit{Instances} & $\phantom{-}0.0223$ & $\phantom{-}0.0051$ & $\phantom{-}0.0044$ & $\phantom{-}0.0080$ & $\phantom{-}0.0294$ \\
            \textit{Features} & $\phantom{-}0.2286$ & $-0.0560$ & $\phantom{-}0.1109$ & $\phantom{-}0.0087$ & $\phantom{-}0.2091$ \\
            \textit{Classes} & $-0.0275$ & $\phantom{-}0.0899$ & $-0.0780$ & $-0.0165$ & $-0.0208$ \\
            \textit{Dimensionality} & $\phantom{-}0.2027$ & $-0.0323$ & $\phantom{-}0.0903$ & $\phantom{-}0.0053$ & $\phantom{-}0.1799$ \\
            \textit{Autocorrelation} & $-0.0906$ & $-0.0621$ & $\phantom{-}0.0002$ & $-0.0508$ & $-0.0746$ \\
            \textit{Minority Class} & $\phantom{-}0.0643$ & $-0.1615$ & $\phantom{-}0.1050$ & $\phantom{-}0.0496$ & $\phantom{-}0.0494$ \\
            \textit{Majority Class} & $\phantom{-}0.0039$ & $-0.0224$ & $\phantom{-}0.0359$ & $-0.0081$ & $\phantom{-}0.0060$ \\
            \textit{Class Entropy} & $-0.0315$ & $\phantom{-}0.0476$ & $-0.0587$ & $-0.0053$ & $-0.0250$ \\
            \hline
        \end{tabular}
    \end{subtable}
    \par\bigskip
    \begin{subtable}{\textwidth}
        \centering
        \caption{Absolute utilization frequency.}
        \begin{tabular}{l|ccccc}
            & \textit{RS} & \textit{HB} & \textit{GA} & \textit{SMAC} & \textit{DS} \\
            \hline
            \textit{Instances} & $\phantom{-}0.0188$ & $\phantom{-}0.0155$ & $\phantom{-}0.0038$ & $\phantom{-}0.0179$ & $\phantom{-}0.0284$ \\
            \textit{Features} & $\phantom{-}0.0724$ & $-0.1501$ & $-0.0533$ & $-0.0782$ & $\phantom{-}0.0563$ \\
            \textit{Classes} & $\phantom{-}0.0349$ & $\phantom{-}0.1146$ & $\phantom{-}0.0026$ & $\phantom{-}0.0328$ & $\phantom{-}0.0482$ \\
            \textit{Dimensionality} & $\phantom{-}0.0660$ &$ -0.1232$ & $-0.0531$ & $-0.0714$ & $\phantom{-}0.0464$ \\
            \textit{Autocorrelation} & $\phantom{-}0.0084$ & $-0.0103$ & $\phantom{-}0.0254$ & $-0.0214$ & $\phantom{-}0.0075$ \\
            \textit{Minority Class} & $\phantom{-}0.0184$ & $-0.1662$ & $\phantom{-}0.0001$ & $-0.0006$ & $\phantom{-}0.0075$ \\
            \textit{Majority Class} & $-0.0730$ & $-0.0694$ & $-0.0276$ & $-0.0574$ & $-0.0762$ \\
            \textit{Class Entropy} & $\phantom{-}0.0496$ & $\phantom{-}0.0938$ & $\phantom{-}0.0214$ & $\phantom{-}0.0499$ & $\phantom{-}0.0619$ \\
            \hline
        \end{tabular}
    \end{subtable}
    \par\bigskip
    \begin{subtable}{\textwidth}
        \centering
        \caption{Optimizer utilization rank.}
        \begin{tabular}{l|ccccc}
            & \textit{RS} & \textit{HB} & \textit{GA} & \textit{SMAC} & \textit{DS} \\
            \hline
            \textit{Instances} & $-0.0259$ & $\phantom{-}0.0030$ & $\phantom{-}0.0171$ & $\phantom{-}0.0202$ & $-0.0319$ \\
            \textit{Features} & $-0.0163$ & $\phantom{-}0.1286$ & $\phantom{-}0.0418$ & $\phantom{-}0.0279$ & $-0.0750$ \\
            \textit{Classes} & $-0.0043$ & $-0.0894$ & $\phantom{-}0.0673$ & $\phantom{-}0.0168$ & $-0.0065$ \\
            \textit{Dimensionality} & $-0.1409$ & $\phantom{-}0.0989$ & $\phantom{-}0.0432$ & $\phantom{-}0.0190$ & $-0.0520$ \\
            \textit{Autocorrelation} & $\phantom{-}0.0167$ & $\phantom{-}0.0548$ & $-0.0415$ & $-0.0074$ & $-0.0219$ \\
            \textit{Minority Class} & $-0.0745$ & $\phantom{-}0.1617$ & $-0.0381$ & $-0.0449$ & $-0.0273$ \\
            \textit{Majority Class} & $\phantom{-}0.0485$ & $\phantom{-}0.0253$ & $-0.0613$ & $\phantom{-}0.0047$ & $\phantom{-}0.0113$ \\
            \textit{Class Entropy} & $\phantom{-}0.0191$ & $-0.0541$ & $\phantom{-}0.0598$ & $\phantom{-}0.0043$ & $-0.0153$ \\
            \hline
        \end{tabular}
    \end{subtable}
\end{table}

The second evaluated aspect of frankensteins mechanics was the influence study of the two main components, i.e. model selection via MCTS and model configuration via the warmstarted optimizer ensembles.\newline
This was evaluated via the three variants of frankensteins automl for three different timeouts in the context of 30 random seeds and the same ten datasets.
An analysis for significant improvement or deterioration within one timeout value, was here done in the form of a two-sample \textit{t}-tests with $p = 0.05$ as well.\newline
All accuracy score averages as well as the significance analysis results can be seen in table~\ref{table:influence-results}.

\begin{table}[ht]
    \caption[Results of the experiments for comparing the three frankenstein variants over longer timeouts.]{
        Results of the experiments for comparing the three frankenstein variants over longer timeouts.
        The individual cells have the same layout as in table~\ref{table:benchmark-results}.
    }
    \label{table:influence-results}
    \renewcommand{\arraystretch}{1.25}
    \begin{subtable}{\textwidth}
        \centering
        \caption{Timeout: 1h}
        \begin{tabular}{l|ccc}
            & \texttt{frankenstein}  & \texttt{frankenstein-rs}  & \texttt{frankenstein-mcts} \\
            \hline
            \texttt{amazon} & $ 0.9735 \pm 0.0049 (30) \phantom{\downarrow}$ & $ 0.9752 \pm 0.0026 (28) \phantom{\downarrow}$ & $ 0.9748 \pm 0.0020 (28) \phantom{\downarrow}$\\
            \texttt{car} & $ 0.9789 \pm 0.0426 (30) \phantom{\downarrow}$ & $ 0.9871 \pm 0.0148 (30) \phantom{\downarrow}$ & $ 0.9633 \pm 0.0674 (30) \phantom{\downarrow}$\\
            \texttt{cifar} & $ 0.3140 \pm 0.0621 (28) \phantom{\downarrow}$ & $ 0.2841 \pm 0.0632 (28) \phantom{\downarrow}$ & $ 0.2664 \pm 0.0768 (30) \downarrow$\\
            \texttt{dexter} & $ 0.9004 \pm 0.0453 (28) \phantom{\downarrow}$ & $ 0.9020 \pm 0.0552 (28) \phantom{\downarrow}$ & $ 0.9090 \pm 0.0426 (29) \phantom{\downarrow}$\\
            \texttt{dorothea} & $ 0.9245 \pm 0.0136 (30) \phantom{\downarrow}$ & $ 0.9183 \pm 0.0235 (29) \phantom{\downarrow}$ & $ 0.9197 \pm 0.0140 (29) \phantom{\downarrow}$\\
            \texttt{krvskp} & $ 0.9933 \pm 0.0023 (27) \phantom{\downarrow}$ & $ 0.9872 \pm 0.0301 (30) \phantom{\downarrow}$ & $ 0.9931 \pm 0.0025 (29) \phantom{\downarrow}$\\
            \texttt{semeion} & $ 0.9235 \pm 0.0588 (30) \phantom{\downarrow}$ & $ 0.9317 \pm 0.0452 (30) \phantom{\downarrow}$ & $ 0.9221 \pm 0.0533 (30) \phantom{\downarrow}$\\
            \texttt{waveform} & $ 0.8412 \pm 0.0278 (29) \phantom{\downarrow}$ & $ 0.8420 \pm 0.0313 (30) \phantom{\downarrow}$ & $ 0.8124 \pm 0.1066 (30) \phantom{\downarrow}$\\
            \texttt{wine} & $ 0.5292 \pm 0.1629 (28) \phantom{\downarrow}$ & $ 0.5709 \pm 0.1327 (27) \phantom{\downarrow}$ & $ 0.6218 \pm 0.0890 (29) \uparrow$\\
            \texttt{yeast} & $ 0.5752 \pm 0.0592 (30) \phantom{\downarrow}$ & $ 0.5594 \pm 0.0822 (28) \phantom{\downarrow}$ & $ 0.5323 \pm 0.0933 (29) \downarrow$\\
            \hline
        \end{tabular}
    \end{subtable}
    \par\bigskip
    \begin{subtable}{\textwidth}
        \centering
        \caption{Timeout: 6h}
        \begin{tabular}{l|ccc}
            & \texttt{frankenstein}  & \texttt{frankenstein-rs}  & \texttt{frankenstein-mcts} \\
            \hline
            \texttt{amazon} & $ 0.9736 \pm 0.0087 (29) \phantom{\downarrow}$ & $ 0.9716 \pm 0.0125 (29) \phantom{\downarrow}$ & $ 0.9743 \pm 0.0043 (29) \phantom{\downarrow}$\\
            \texttt{car} & $ 0.9714 \pm 0.0577 (30) \phantom{\downarrow}$ & $ 0.9926 \pm 0.0068 (30) \phantom{\downarrow}$ & $ 0.9729 \pm 0.0602 (30) \phantom{\downarrow}$\\
            \texttt{cifar} & $ 0.3048 \pm 0.0652 (30) \phantom{\downarrow}$ & $ 0.3025 \pm 0.0682 (29) \phantom{\downarrow}$ & $ 0.3210 \pm 0.0542 (30) \phantom{\downarrow}$\\
            \texttt{dexter} & $ 0.8904 \pm 0.1030 (29) \phantom{\downarrow}$ & $ 0.9044 \pm 0.0600 (29) \phantom{\downarrow}$ & $ 0.9222 \pm 0.0255 (29) \phantom{\downarrow}$\\
            \texttt{dorothea} & $ 0.9094 \pm 0.1078 (29) \phantom{\downarrow}$ & $ 0.9086 \pm 0.1108 (30) \phantom{\downarrow}$ & $ 0.9218 \pm 0.0173 (30) \phantom{\downarrow}$\\
            \texttt{krvskp} & $ 0.9930 \pm 0.0022 (29) \phantom{\downarrow}$ & $ 0.9920 \pm 0.0086 (30) \phantom{\downarrow}$ & $ 0.9926 \pm 0.0022 (30) \phantom{\downarrow}$\\
            \texttt{semeion} & $ 0.9070 \pm 0.1576 (30) \phantom{\downarrow}$ & $ 0.9487 \pm 0.0206 (30) \phantom{\downarrow}$ & $ 0.9093 \pm 0.0860 (30) \phantom{\downarrow}$\\
            \texttt{waveform} & $ 0.8500 \pm 0.0241 (30) \phantom{\downarrow}$ & $ 0.8480 \pm 0.0322 (29) \phantom{\downarrow}$ & $ 0.8362 \pm 0.0423 (30) \phantom{\downarrow}$\\
            \texttt{wine} & $ 0.6293 \pm 0.0985 (29) \phantom{\downarrow}$ & $ 0.6510 \pm 0.0403 (28) \phantom{\downarrow}$ & $ 0.6326 \pm 0.0612 (30) \phantom{\downarrow}$\\
            \texttt{yeast} & $ 0.5759 \pm 0.0758 (30) \phantom{\downarrow}$ & $ 0.5697 \pm 0.0892 (30) \phantom{\downarrow}$ & $ 0.5850 \pm 0.0648 (29) \phantom{\downarrow}$\\
            \hline
        \end{tabular}
    \end{subtable}
    \par\bigskip
    \begin{subtable}{\textwidth}
        \centering
        \caption{Timeout: 12h}
        \begin{tabular}{l|ccc}
            & \texttt{frankenstein}  & \texttt{frankenstein-rs}  & \texttt{frankenstein-mcts} \\
            \hline
            \texttt{amazon} & $ 0.9752 \pm 0.0022 (30) \phantom{\downarrow}$ & $ 0.9737 \pm 0.0039 (30) \phantom{\downarrow}$ & $ 0.9756 \pm 0.0015 (30) \phantom{\downarrow}$\\
            \texttt{car} & $ 0.9838 \pm 0.0187 (30) \phantom{\downarrow}$ & $ 0.9940 \pm 0.0057 (30) \uparrow$ & $ 0.9766 \pm 0.0556 (30) \phantom{\downarrow}$\\
            \texttt{cifar} & $ 0.3316 \pm 0.0467 (30) \phantom{\downarrow}$ & $ 0.3319 \pm 0.0497 (30) \phantom{\downarrow}$ & $ 0.3069 \pm 0.0645 (30) \phantom{\downarrow}$\\
            \texttt{dexter} & $ 0.9265 \pm 0.0154 (30) \phantom{\downarrow}$ & $ 0.9296 \pm 0.0128 (30) \phantom{\downarrow}$ & $ 0.9285 \pm 0.0127 (29) \phantom{\downarrow}$\\
            \texttt{dorothea} & $ 0.9303 \pm 0.0136 (30) \phantom{\downarrow}$ & $ 0.9003 \pm 0.1494 (30) \phantom{\downarrow}$ & $ 0.9242 \pm 0.0205 (30) \phantom{\downarrow}$\\
            \texttt{krvskp} & $ 0.9928 \pm 0.0042 (30) \phantom{\downarrow}$ & $ 0.9933 \pm 0.0025 (29) \phantom{\downarrow}$ & $ 0.9904 \pm 0.0113 (30) \phantom{\downarrow}$\\
            \texttt{semeion} & $ 0.9501 \pm 0.0173 (30) \phantom{\downarrow}$ & $ 0.9411 \pm 0.0346 (30) \phantom{\downarrow}$ & $ 0.9498 \pm 0.0236 (30) \phantom{\downarrow}$\\
            \texttt{waveform} & $ 0.8547 \pm 0.0106 (30) \phantom{\downarrow}$ & $ 0.8575 \pm 0.0165 (30) \phantom{\downarrow}$ & $ 0.8551 \pm 0.0095 (30) \phantom{\downarrow}$\\
            \texttt{wine} & $ 0.6574 \pm 0.0404 (30) \phantom{\downarrow}$ & $ 0.6508 \pm 0.0793 (30) \phantom{\downarrow}$ & $ 0.6491 \pm 0.0764 (30) \phantom{\downarrow}$\\
            \texttt{yeast} & $ 0.6028 \pm 0.0213 (30) \phantom{\downarrow}$ & $ 0.5995 \pm 0.0209 (29) \phantom{\downarrow}$ & $ 0.5981 \pm 0.0245 (30) \phantom{\downarrow}$\\
            \hline
        \end{tabular}
    \end{subtable}
\end{table}

\section{Analysis of the Experiment Outcomes}
\label{sec:evaluation:analysis}
This collected data is now analysed in more detail and used to answer the two main research questions of this thesis.

\subsection{Feasibility of AutoML via an Optimizer Ensembles}
\label{sec:evaluation:analysis:feasibility}
The first research question is about the feasibility of this optimizer ensembles approach for AutoML problems.\newline
As observed in the  tables~\ref{table:benchmark-results} and~\ref{table:significanse-counts}, this approach is partially competitive to the compared state-of-the-art approaches.
It appears for the evaluated datasets inferior to auto-sklearn and TPOT, where it was only able to achieve competitive scores without a significant deterioration for three respectively six datasets but no significant improvements.
Compared to ML-Plan and Mosaic, the benchmark appears more balanced.
It shows a minor advantage compared to ML-Plan with significant improvements for six datasets and minor disadvantages compared Mosaic with significant improvements for only one dataset.\newline
Based on this results, the current reference implementation of the approach of this thesis shows no improvements against the established state-of-the-art approaches auto-sklearn and TPOT.
However, it was able to yield partially competitive results and this could indicate the ability to close up to the current state-of-the-art with further developments, refinements and future work.

As part of the feasibility question, it is also evaluated which influence the two core components of this approach, i.e. model selection and optimizer selection as a Bandit problem tackled via MCTS as well as model configuration approached with warmstarted optimizer ensembles, have on the results.
The results of this evaluation can be found in table~\ref{table:influence-results}.\newline
For this experiments, one of the components was replaced with a alternative default behavior.
At first, a random search instead of MCTS but all optimizers included.
Second, only Bayesian optimization but reached via a MCTS.
Both were seen as competitors to the unmodified approach with a MCTS and all optimizers.\newline
Unfortunately, across all three evaluated timeouts there are only four occurrences of one of the modified variants having significant different scores compared to the unmodified and complete variant.
Additionally, these four significant differences do not suggest any kind of pattern and are most likely just outliers.
Therefore, based on the evaluated datasets and timeouts, the approach appears to be powered by a combination of both components and is not clearly influenced by one more than the other.

All in all, the current reference implementation does not indicate a feasibility of the optimizer ensemble approaches.
It has to be noted that auto-sklearn and TPOT haven been in development for a significantly longer time.
But the results were at least partially competitive, such that it is not completely ruled out, that this idea can become feasible with additional research after this reference implementation reaching a comparable degree of maturity and optimization as the two highlighted state-of-the-art approaches.\newline
The apparently similar influence of both components on the overall behavior does not give a clear suggestion, on which of these two components this future research should be focussed.
More evaluations and experimentation with modifications and additions in both components will be necessary.

\subsection{Optimizer Utilization Frequencies}
\label{sec:evaluation:analysis:optimizer}
Since a MCTS as a method to tackle an Multi-Armed Bandit problem it used to select the optimizer, it will try to explore the most suitable optimizer for each problem class/instance and exploit it.+
Hence, if the timeout is long enough and MCTS was able to perform a sufficient exploration, there should be a clearly visible trend towards one or more optimizer for the problem class, i.e. an input dataset.\newline
This inherent mechanism of gathering indications about the suitability of an optimization algorithm towards the optimization of parametrizations of constructed pipelines for different datasets was the basis for the second research question.
It should be examined it there are recognizable trends or tendencies towards one or more optimization algorithm across all datasets or otherwise, if it is correlated to properties of the corresponding dataset which optimizer was utilized with which frequency.

As presented in table~\ref{table:optimizer-calls}, SMAC, i.e. a implementation of Bayesian optimization, was utilized most often for each of the datasets.
Although Hyperband and the Genetic Algorithm came more or less close regarding the relative utilization frequency, they appear to be selected less favorably by the MCTS.
Both Random Search as well as Discretization Search trail significantly far behind the other three and have really low utilization frequency with a maximum around $15\%$.\newline
If a ranking is created along the five overall relative utilizations it would look like the following:
\begin{enumerate}
    \item SMAC
    \item Genetic Algorithm
    \item Hyperband
    \item Discretization Search
    \item Random Search
\end{enumerate}
This ranking is interesting in two ways.
First, since Hyperband is more or less a more sophisticated Random Search, the three least suitable optimization algorithms are all three based on search algorithms.
Second, auto-sklearn uses internally SMAC, TPOT a Genetic Algorithm and ML-Plan a more complex form of Discretization Search.
When this ranking is now compared to the results of this three AutoML approaches in table~\ref{table:benchmark-results} there is a similarity as auto-sklearn and TPOT being at the top with comparable scores, but clearly superior to ML-Plan for the evaluated datasets in this experiment series.
But this is probably all in all solely a coincidence and not caused by the actual choice of optimizer, because the three perform more competitive in other benchmark evaluations~\cite{Mohr-ML-Plan} then in this evaluation.

Since there is a clear trend recognizable regarding the most favorable optimizer choices of the MCTS across all datasets of this experiment, the evaluation regarding correlations between dataset properties and optimizer utilization becomes less relevant, but was done nevertheless.
As presented in table~\ref{table:influence-results}, there are only weak correlations coefficients between all dataset properties and optimizer utilizations across all three utilization metrics.
The correlation coefficients are overall in the range $-0.1662 \leq \rho \leq 0.2286$, which does not suggest any significant correlation altogether.

This results suggest that SMAC is the most suitable choice of optimization algorithm across all regarded datasets independently of the eight evaluated dataset properties.
If this is a trend beyond this evaluation, has to be controlled in a evaluation with a much higher amount of datasets.
