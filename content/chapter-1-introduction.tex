% !TEX root = ../my-thesis.tex
%
\chapter{Introduction}
\label{sec:intro}

\section{Motivation}
\label{sec:intro:motivation}

Since machine learning is present in a fast growing part of everyday life, economy and science, the number of people who want to use machine learning and apply it to their use case is equally fast growing.
The problem is that using machine learning beyond simple examples and tutorials requires theoretical domain knowledge as well as solid programming skills.\newline
Applying a machine learning approach includes the selection of a suitable learning model or even multiple models and other components combined to a complex pipeline.
The cautious choice of machine learning algorithms for each problem or use-case is crucial to have a suitable and well performing model.
Each selected machine learning algorithm may expose hyperparameters in return and the configuration of these parameters is a second necessary step for the application of machine learning.
Finally, the selected model or pipeline with the configured parameters must be actually implemented with a suitable programming language within a machine learning framework or toolbox, which are usually comparably big and complex components.
Therefore, caused by these three necessary steps there is a learning and knowledge barrier that prevents interested people with a non-technical background from using machine learning.

In the last years, different approaches were developed, which can take this barrier off their shoulders and empower nearly everyone to use machine learning for datasets of their domain or setting.
These approaches allow this by automating this algorithm configuration problem, consisting of the described sub-problems: Selecting models, respectively components of pipelines, configuring all required parameters and constructing a usable implementation.
All the end-user has to do is to provide their data in a suitable dataset format and usually specify a resource or time budget for the solver of the algorithm configuration problem.\newline
Within this budget, the best pipeline can be searched regarding a metric, as for example the predictive accuracy.
Hence, this problem of finding a machine learning pipeline for a dataset can be considered an optimization problem.
Solving this optimization problem is the challenge of a research area called \textit{Automated Machine Learning} or short \textit{AutoML}.

AutoML received much attention in the scientific machine learning community and a broad spectrum of different methods were developed and published.
The majority of these approaches and the plethora of different strategies they utilize, have usually one thing in common.
They see the model selection and the model configuration as a joint optimization problem and therewith solve this two problems within a single optimization space with a single optimization algorithm.\newline
However, recently a few new approaches were published that separate this two optimization spaces and utilize different optimization methods for the two respective spaces.
But this can application of more than one optimization algorithm can be extended two three or more optimizers and in theory, this should show an improvement because of the \textit{No-free-lunch Theorems for optimization}.
It states that when an optimization algorithm performs superior for one problem or class of problems it has to pay for this by performing inferior for other problems.
In the context of AutoML this would imply that one optimization approach, can not yield the best solution across all datasets and domains.\newline
Combining multiple optimization methods into an ensemble of optimizers would allow this ensemble to optimize each possible pipeline with the most suitable optimization algorithm with respect to the parameter space of this pipeline and the properties of the dataset.
Therewith, the most suitable optimization algorithm out of the ensemble could be identified and exploited to the given AutoML problem setting.\newline
For creating and utilizing this optimizer ensembles in the context of AutoML, a novel approach is required and will be the topic of this thesis. 

\section{Thesis Goal and Research Questions}
\label{sec:intro:goal}
In the context of this thesis, a feasible concept and algorithm for such optimizer ensembles will be devised and exemplarily realized as a reference implementation.
This  implemented optimizer ensemble algorithm will be the test subject for a series of empirical evaluations.
Overall, the goal of this thesis is to answer two research questions regarding this approach for optimizer ensembles in the focus of AutoML with the data gathered during the evaluations:
\begin{enumerate}
    \item Is the devised approach for optimizer ensembles feasible in the context of AutoML?
    More precisely, is the result quality comparable or even better compared to other state-of-the-art approaches?
    And if it is a feasible approach, what influence have the particular components of the approach?
    \item Can knowledge about the optimization in the general AutoML context be extrapolated from this approach?
    When the frequencies of utilizing certain optimizations algorithms during the execution of this approach are recorded, this may be indications regarding their capability for different AutoML problems, since this approach tries to find the best suited optimizer out of the ensemble for the input problem and exploit it.
    Based on this frequencies, is every optimizer called an equal amount of times or is one or more optimization method favoured for the AutoML use case?
    Does it depend on certain dataset properties of the AutoML setting, whether an optimization method is used relatively often?
\end{enumerate}

\section{Thesis Structure}
\label{sec:intro:structure}
Following this introduction, this thesis is structured into five chapters.
The content and goals of each chapter are briefly explained here:

\textbf{Chapter \ref{sec:theory}} \\[0.2em]
At first, the foundations of the machine learning, the AutoML setting and optimization in general are elucidated, to give a starting point for the related work part of this chapter and aid the understanding of the approach of this thesis.
A selection of Black Box optimization algorithms is presented and for each a few existing AutoML approaches, which apply this algorithm.
With the help of the No-Free-Lunch theorems for optimization, it will be argued why it is not sufficient to utilize a single optimization method for a single AutoML optimization space.
The chapter is concluded by presenting a few more AutoML approaches that already apply the idea of separating the single big optimization space and applying more than one optimizer as supplementary related works besides the AutoML approaches, which were presented priorly altogether with their optimization algorithm.

\textbf{Chapter \ref{sec:approach}} \\[0.2em]
The approach of this thesis is theoretically specified here and the underlying design choices explained.
Every concept from the theory chapter, which is taken up again for the specification of this approach is explained in more detail.
All improvements from applying an optimizer ensemble instead of a single optimizer are listed with reference to the disadvantages from the related work approaches mentioned in chapter~\ref{sec:theory}.

\textbf{Chapter \ref{sec:implementation}} \\[0.2em]
To apply the approach for actual AutoML problem settings as well as evaluating the approach empirically an implementation is necessary.
The technical details of an exemplaric implementation as a library developed with the Python programming language are explicated accompanied with an overview of the utilized libraries.

\textbf{Chapter \ref{sec:evaluation}} \\[0.2em]
In order to answer the previously listed research questions a series of empirical experiments is designed and conducted.
This approach is matched up against state-of-the-art score of some of the related work approaches as well as altered versions of itself.
Structured by the addressed research question, the experiments design and execution is explained.
Subsequent to this explanations, the results of the experiments are shown and interpreted with regard to the research questions.

\textbf{Chapter \ref{sec:conclusion}} \\[0.2em]
Finally, the outcomes of the research questions are interpreted to compare this approach to other AutoML approaches.
The answers to the research questions are used to summarize the concepts of this approach and their validity.
At last, an outlook is given about additions, which could further improve this approach and are a starting point for future work.
