% !TEX root = ../my-thesis.tex
%
\chapter{Theoretical Foundations}
\label{sec:theory}
Before presenting the approach for tackling AutoML with an ensemble of optimizers, some theoretical foundations of both elements, AutoML and optimization, are given in the following.
This theoretical background is structured in three parts:
\begin{enumerate}
    \item Some basic concepts and intuitions of machine learning in general are outlined alongside with the challenges and problems that arise when applying machine learning.
    \item The concepts and usual approaches of AutoML are introduced, which were developed to tackle the listed challenges of classical machine learning. In addition, the connection between the AutoML setting and typical optimization problems is illustrated.
    \item As the foundation for building an ensemble of optimizers a selection of established optimization methods is given and explained.
\end{enumerate}
The overview of optimization methods is concluded with the discussion of a theoretical drawback of using a single optimization method.
Before explaining the ensemble approach in the next chapter, which could counter this drawback to a certain degree, this discussion is continued with a selection of related works publications, where other approaches that addressed this theoretical disadvantage of using a single optimization method are mentioned. 

\section{Classical Machine Learning}
\label{sec:theory:ml}
Machine learning in general has been a topic in computer science and mathematics for several decades even before attempts to autotomize it  in the form of AutoML were made.
AutoML is of course not replacing this classical approach, but it is an extension of it.
Therefore, a short overview from a more abstract and theoretical point of view of this classical machine learning without AutoML is given in this first theory section.
Especially supervised learning is elucidated here and one possible definition is given since the approach of this thesis will focus on classification settings, which are a sub-group of supervised learning settings.
With this foundation, AutoML is explained in the following sections.

If a human is given a task where the correct solution or reaction is not evident, a human has always the possibility to react with a random solution or with an arbitrary reaction.
But if the human has any prior first- or second-hand experience with the same or a similar task, the human can choose the reaction based on the memories of different outcomes for different reactions for the more or less similar prior task.
With the high abstraction level and very symbolic nature of human thinking and memorization, it is comparably easy for humans to recognize even remote similarities between tasks.\newline
This is very challenging for a computer in contrast because the models of tasks, experiences, and outcomes of reactions to tasks have to be readable and understandable for a machine, i.e. be in any kind of structured and consistent format.
This setting was formalized by~\textcite{Mitchell-MachineLearning} as a combination of $T, P$ and $E$, where $T$ is a class of tasks, $P$ a performance measurement for solutions of a specific instances of the tasks class $T$ and $E$ is either given or collected experience, i.e. performance measurements for certain solutions in the context of specific task instance $t\in T$.\newline
Of course it is possible for a human programmer to manually specify the solution with the best performance measurement for any possible $t\in T$ but for a high $|T|$ this is rarely possible and viable.
Here, machine learning has its use-case: "Machine learning enables us to tackle tasks that are too difficult to solve with fixed programs written and designed by human beings"~\cite{Goodfellow-DeepLearning}.

A high number of different types of task classes $T$ is imaginable, but one of the most common ones in the machine learning field is \textit{Supervised Learning}.
In supervised learning, $T$ includes a fix set of all possible solutions $S$.
The concrete task for Supervised Learning is now to select for a given $t\in T$ a $s\subseteq S$ such that the performance measurement $P(t,s)$ is optimal.
To enable a machine to learn supervised, the experience $E$ has to be successively build in the form of $\{(t_1, s_1, P(t_1, s_1)), ..., (t_n, s_n, P(t_n s_n))\}$.\newline
For a new task instance $t_i$, the computer will select an $s_i$ based on a decision model build from $E$, which is supposed to be at least better than random guessing.
Depending on the selected $s_i$, the computer will receive a performance feedback $P(t_i, s_i)$ and finally enrich $E$ with $(t_i, s_i, P(t_i, s_i))$ as well as updating the decision model with the changed $E$.
Therewith, a well working machine learning algorithm for supervised learning would now be able to achieve $P(t_j, s_j) \geq P(t_i, s_i)$ if $t_j$ and $t_i$ are similar instances, since it already has experience values which selection for $s_i\subseteq S$ had achieved a certain performance value for $t_i$ and might therefore be a good or respectively bad choice for $t_j$ such that $s_j$ can be deduced to a certain degree.

This task of comparing and judging the similarity of different instances of $T$ and to build a practicable decision model based on $E$ to select a solution out of $S$ has been tackled with a plethora of different algorithms or even combinations of multiple algorithms as a form of machine learning pipeline.
Usually, these algorithms have to be configured with a set of hyperparameters depending on $T$ and often $T$ also has to be transformed before presenting concrete instances to the algorithm for learning, i.e. pre-processing each $t\in T$ with one or more transformation methods.

\section{Automated Machine Learning}
\label{sec:theory:automl}
Automated Machine Learning, or short \textit{AutoML}, tries to automate most tasks of the process of creating a machine learning application and also other associated tasks from the data science field.
This serves two main purposes:
\begin{itemize}
    \item By automating construction and configuration, it tackles the knowledge barrier, which prevents interested people from applying machine learning.
    \item By reducing the amount of manual work to a bare minimum, it offers a big speed-up for applying machine learning regardless of the level of knowledge of the user.
\end{itemize}
In this section, a description of the general workflow of an AutoML tool is given and the usual two main steps of such an AutoML workflow are illustrated.
This will be concluded with a formalization of the AutoML problem setting as an optimization problem.

\subsection{General Workflow}
\label{sec:theory:automl:workflow}
AutoML applications are usually designed very homogeneously and therefore have very similar workflows among themselves.
As an input, the application receives data in a machine-readable format and usually some form of constraint for its execution, such as hardware resource or time limitations.\newline
The expected output is the best found machine learning pipeline for the given data measured by some task-dependent metric and additionally often the actual performance value of the found machine learning pipeline measured in said metric.
Additionally, each AutoML tool can have further necessary configurations as an input.\newline
The constraints for execution are required to prevent the AutoML tool from searching for the best machine learning pipeline for the given data virtually forever with probably continuously growing hardware consumption.
Therefore, the  execution time and/or some hardware resources are constrained in the form of budgets.
These inputs and outputs of a conceptual AutoML tool are illustrated in Fig.~\ref{fig:theory:conceptualAutoMLTool}.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=\textwidth]{gfx/Figures/Theory/AutoMLTool.pdf}
    \caption{High-level view of a conceptual AutoML tool.}
	\label{fig:theory:conceptualAutoMLTool}
\end{figure}

The general workflow of an AutoML tool, as numbered and illustrated in the figure, can be generalized with the following steps:
\begin{enumerate}
    \item The AutoML tool gets some input data and execution constraints as an input.
    \item Model selection and model configuration are repeated until execution budgets are spent.
    \item Model selection as well as model configuration can perform a model evaluation to get a score for any pipeline candidate they produce.
    \item When the user-specified constraints are reached, for instance a timeout is hit, the best evaluated pipeline as well as the score of this pipeline are returned as an output.
\end{enumerate}
The model evaluation step is usually performed by training a pipeline on one split of the input data and testing it with another split of the data, sometimes called validation data, while observing some kind of performance measurement, for example the accuracy of predictions or some error metric.
But other evaluation techniques, for example a cross-validation, are also common.\newline
While the model evaluation step is only very loosely defined and usually not a complex procedure, the model selection and model configuration steps of the AutoML workflow are more sophisticated.
They have to select a suitable candidate from one or more, usually large, spaces. 
Both spaces, for model selection as well as for model configuration, will be individually explained in the following.

\subsection{Model Selection}
\label{sec:theory:automl:selection}
Model selection is the task of selecting $1$ to $n$ components that will be part of the machine learning pipeline and sequentially traversed when data is passed through the pipeline.
For example, a simple, but valid pipeline would be to apply a Principal Component Analysis on the data as a first component and to use a Support Vector Machine for classification on the processed data as a second component afterwards.
In this step of the workflow, the space one candidate has to be selected from, consists of all these valid pipeline component combinations, which usually consist of two or more components.\newline
Usually, there are three types of components, although other classifications of the components are possible as well:
\begin{itemize}
    \item Pre-Processing: Transform the input data before presenting it to the learning algorithm.
    \item Learning Algorithm: Perform the actual machine learning task on the data, for example classify a datapoint after training.
    \item Post-Processing: Transform the output of the learning algorithm before yielding it as a final result.
\end{itemize}
Although in theory, only a single learning algorithm is necessary and components for pre- and post-processing are optional, having at least a pre-processing component is very common in machine learning pipelines and included most of the time.\newline
It is possible to use more than one component of each of the three types in a single pipeline, such that pipelines with arbitrary complexity and size can be created.
More than one pre- or post-processor can be used in sequence or in parallel with some kind of aggregator component subsequently.
Similarly, more than one learning algorithm can be combined to be used as a composite or ensemble model with a proper aggregation method as for example Bagging of models~\cite{Breiman-BaggingPredictors}.
An example of such a more complex pipeline is illustrated in Fig.~\ref{fig:theory:complexPipeline}.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=\textwidth]{gfx/Figures/Theory/ComplexPipeline.pdf}
    \caption[Example of a more complex machine learning pipeline.]{Example of a more complex machine learning pipeline with four pre-processing components and an ensemble learner. The pre-processors are shown in blue: A One-hot Encoder as well as a subsequent Feature Union of a PCA and an LSA. The learning algorithm in green is a Majority Voter ensemble as an aggregation of a Decision Tree, an SVM and a Logistic Regression classifier.}
	\label{fig:theory:complexPipeline}
\end{figure}

Model selection, where the resulting pipelines can have this complexity, needs to add a structural relationship between the selected pipeline components.
Therewith, a valid model selection result can be seen as a simple formalization consisting of two elements:
\begin{itemize}
    \item With a given set of all possible components $C$ a list of components $C' = \langle c_1, ..., c_r\rangle$ is created with $\forall c_i, i \in [1, r] : c_i \in C$.
    The components of that list are the selected components that will be used to construct the pipeline.
    \item A binary structure relation $\prec \subset C' \times C'$. It defines which components are a data input for another component to define an order of the components for the pipeline, for instance $(c_a, c_b) \in \prec$. This is also possible for aggregation components or other components with multiple expected inputs by adding all inputs individually. For example, with an aggregation component $c_a$ and three input components $c_i, c_j, c_k$, this could be $\{(c_i, c_a), (c_j, c_a), (c_k, c_a)\} \subset \prec$. 
\end{itemize}
Nevertheless, to find valid results for $C'$ and $\prec$ is not a trivial task.
For instance, when an aggregator for multiple parallel pre-processing components is needed, a valid choice could be a Feature Union component but a Decision Tree component would be invalid for this pipeline position.\newline
Therefore, if the pipelines shall be allowed to be more complex than for example a single pre-processing component and a single learning algorithm, there is depending on $C'$ a probably big set of constraints for the binary structure relation $\prec$.

\subsection{Model Configuration}
\label{sec:theory:automl:configuration}
With the component list $C'=\langle c_1, ..., c_r\rangle$ and the associated binary structure relationship $\prec$ as the two results of the model selection step, the pipeline is not ready to be used yet.
Usually each component of $c_k \in C'$ has a set of hyperparameters and for each one a concrete value is needed, which is the task of the model configuration step.\newline
The candidate space for the model configuration step is completely dependent on the resulting $C'$.
Each component $c_k$ has a set of hyperparameters $\Theta_k=\{ \theta_{k_1}, ..., \theta_{k_j} \}$ where each hyperparameter has a range of valid values for this hyperparameter from which values have to be chosen from.
Such a range can for example be a numeric set like $\mathbb{N}$ or a set with custom values as for example $\{\textrm{true}, \textrm{false}\}$.

When combining all ranges $\bigcup\limits_{i=1}^r \Theta_i$, a parametrization space with dimension $r$ is defined as the candidate space for the model configuration step, where configuration vectors can be drawn from.
If there are no additional constraints attached to the possible configurations, all vectors of this space represent valid candidates and will result in a working pipeline instance created from $C'$ and configured with this vector.\newline
When such a pipeline is constructed and configured with the vector values it can finally be evaluated as a candidate pipeline.
An example for a concrete parametrization drawn from a three dimensional parameter space is shown in Fig.~\ref{fig:theory:parameterSpace}
\begin{figure}[ht!]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
          view={35}{15},
          axis lines=center,
          xtick={0.25, 0.5, 0.75},ytick={5,10, 15},ztick={-10,-5,5,10},
          xmin=0,xmax=1,ymin=0,ymax=20,zmin=0.5,zmax=2.5,
          zticklabels={true, false},ztick={1,2},
          z tick label style={anchor=east}]
        ]
        \addplot3 [only marks] coordinates {(0.4,10,1)};
        \addplot3 [no marks,densely dashed] coordinates { (0.4,0,0.5) (0.4,10,0.5) (0.4,10,1)};
        \node [above right] at (axis cs:0.4,10,1) {$(0.4,10,\textrm{true})$};
        \end{axis}
    \end{tikzpicture}
    \caption[An exemplaric parameter configuration space.]{An exemplaric parameter configuration space. One concrete configuration with the values $(0.4,10,\textrm{true})$ inside the parameter space was selected. The overall space has the dimensions $[0..20]$, $[0, 1)$ and $\{\textrm{true}, \textrm{false}\}$.}
	\label{fig:theory:parameterSpace}
\end{figure}

\subsection{Formalization of AutoML as an Optimization Problem}
\label{sec:theory:automl:optimization}
The choices in the model selection and model configuration steps for one candidate out of the corresponding candidate spaces should not be arbitrary.
Evaluations of a candidate pipeline yield a score for this candidate and intuitively a suitable AutoML approach should try to select candidates with scores as good as possible.
Therefore, the AutoML problem is often considered as an instance of an optimization problem and can be formalized as one as well.\newline
\textcite{Thornton-AutoWeka} give a formalization for AutoML problems as a so called \textit{Combined Algorithm Selection and Hyperparameter optimization} problem, or short \textit{CASH} problem.
Their definition is the following:
\begin{equation*}
    A^*, \lambda_* \in \> \underset{A^{(j)} \in \mathcal{A},\lambda \in \Lambda^{(j)}}{\mathrm{argmin}} \> \frac{1}{K} \sum_{i=0}^K \mathcal{L} (A_\lambda^{(j)}, D_{\textit{train}}^{(i)}, D_{\textit{valid}}^{(i)})
\end{equation*}
This formula consists of the following parts:
\begin{itemize}
    \item $A^*$: The best solution of the algorithm selection, i.e. for the AutoML setting the best pipeline construction found in the model selection step
    \item $\lambda_*$: The best hyperparameter configuration for $A^*$ found in the the model configuration step
    \item $\mathcal{A}$: A set of possible algorithms to choose from and given in the form $\mathcal{A} = \{A^{(1)}, ..., A^{(R)} \}$, which is the set of all valid pipeline component combinations in the case of AutoML
    \item $\Lambda^{(j)}$: Each algorithm $A^{(j)} \in \mathcal{A}$ has a parameter configuration domain $\Lambda^{(j)}$, where a concrete parametrization $\lambda$ can be selected from
    \item $\frac{1}{K} \sum_{i=0}^K$: This is used to calculate the K-fold cross-validation of a pipeline, but other evaluation strategies are also possible if the formula is adjusted accordingly
    \item $\mathcal{L} (A_\lambda^{(j)}, D_{\textit{train}}^{(i)}, D_{\textit{valid}}^{(i)})$: This is the loss, calculated with the loss function $\mathcal{L}$, of the algorithm $A^{(j)}$ configured with the parametrization $\lambda$, trained on the $i$-th split of the training data $D_{\textit{train}}^{(i)}$ and evaluated on the validation data $D_{\textit{valid}}^{(i)}$
\end{itemize}
Instead of the integration of the K-fold cross-validation in the definition of \citeauthor{Thornton-AutoWeka} via $\frac{1}{K} \sum_{i=0}^K$, it is of course also possible to replace it with any evaluation method.\newline
In summary, the goal is to select an algorithm together with a parameter configuration, such that the expected loss calculated via a cross-validation is minimal.

\section{Black Box Optimization}
\label{sec:theory:optimization}
With the formalization of AutoML as a CASH problem, the question arises whether it can be tackled similar to standard textbook optimization problems, where established optimization algorithms exist to approximate or to find an optimal solution.
Unfortunately, the CASH problem is a special case of optimization problem called \textit{Black Box} optimization problem.\newline
In the following section, it is explained in what way black box problems are different from the standard textbook optimization problem and why established optimization algorithms cannot be used.
Afterwards, a selection of existing algorithms is presented that can instead be used for black box optimization problems, as for instance the AutoML setting.
Finally, with the aid of the \textit{No-Free-Lunch Theorem}, it will be reasoned why these approaches cannot be optimal and why therefore an ensemble approach could be more promising.

\subsection{Differences to General Optimization Problems}
\label{sec:theory:optimization:differences}
\textcite{Boyd-Optimization} define a standard single-objective optimization problem as the following:
\begin{alignat*}{1}
    \underset{x}{\mathrm{minimize}} \textit{ or } \underset{x}{\mathrm{maximize}} \qquad & f_0(x)\\
    \textit{subject to} \qquad & f_i(x) \leq 0,\> i=1,...,m\\
                        &  h_j(x) = 0,\> j=1,...,p\\
\end{alignat*}
Here is the goal to find a value for the optimization variable $x$ such that $f_o(x)$ has either its minimum or maximum value.
Hereby, $x$ does not have to be a single value but can also be a vector of values.
$f_0$ is usually called an objective function, target function, or cost function.\newline
The choices for $x$ can be limited by a set of inequality constraint functions $f_i$ and a set of equality constraint functions $h_j$.
If $m=p=0$, there are no constraint functions and the optimization problem is called unconstrained.
For such cases, x could be any value from the domain of $f_0$.\newline
If $f_0$ has certain properties, for example if it is a convex function, there are specialized optimization methods that can solve the optimization problem without the derivative $f_0'$.
But for general optimization algorithms, where no properties of $f_0$ besides derivability are expected, $f_0'$ is required to solve the optimization problem analytically.

The problem is, for some optimization problems, the exact formula of $f_0$ is not known or not given and therefore no derivative can be calculated such that the general optimization algorithms are not applicable.
Instead, it is only possible to obtain $f_0(x)$ for any requested $x$ for example by performing an experimental evaluation, looking the result up in a table, or asking an expert.
For such problems it is only possible to observe the inputs and corresponding outputs of $f_0$ but the inner workings are hidden.
$f_0$ is metaphorically a black box, where something goes in and something comes out but it is not possible to look inside, and therefore such optimization problems are called black box optimization problems.

AutoML, formalized as a CASH problem, is a black box optimization problem as well.
It is not possible, or at least not yet realizable, to determine a general formula for $f_0$.
The relationship between the properties of the dataset, the different pipeline components with their configuration parameters and other factors, as for example randomness of some components, is way to complex and inscrutable to be formulated into a single cohesive function.\newline
But it is possible to to get an evaluation of any $x$, i.e. a concrete pipeline with a complete configuration, by training and testing the pipeline with given data and an evaluation metric as for example the prediction accuracy.
The absence of a concrete formula for $f_0$ but the possibility to determine $f_0(x)$ renders AutoML a black box optimization problem.

Without the possibility to determine an optimal or near optimal candidate analytically, as in the case of a black box optimization problem, a series of candidates has to be selected and evaluated while trying to find or at least approximate an optimal solution.
Now the question remains for the AutoML setting, how candidates from the two presumably large candidate spaces of model selection and model configuration should be selected for each iteration of the candidate selection and candidate evaluation loop of the optimization and how such an approximation can be attempted.\newline
A solely random selection in every iteration is not very likely to find good results with such a high number of choices.
Since the approach evaluates several candidates iteratively, a better approach would be to base the selection in the current iteration on an updated view of the optimization space landscape based on the knowledge gathered from previous iterations instead.

Several algorithms were developed to tackle such an iteratively approximation of the optimum of a black box optimization problem.
Some of them have already shown promising results for CASH problems and therefore the current state-of-the-art AutoML research works primarily with the following three optimization strategies:
\begin{itemize}
    \item (Heuristic) Search
    \item Genetic and Evolutionary Algorithms
    \item Bayesian Optimization
\end{itemize}
In general, every black box optimization strategy is defined by a selection criterion for a set of candidates for evaluation out of the candidate space and an order in which they are evaluated.
Hereby, the selection as well as the order can be defined in many ways.
For example, the two could be (partially) pre-defined or random or a mixture of both.
It is also possible that the combination of both, definition of candidates as well as order of candidate evaluations, is defined implicitly by the prior selected candidates.
In this case, selecting one more candidate for the set would depend on the already selected candidates in the set as well as knowledge about the optimization landscape gathered from them, and with this stepwise extension the order of evaluations would be implicitly defined.

For each one of the aforementioned black box optimization strategies, this candidate selection and candidate order will be explained in general and on the basis of some concrete approaches from the AutoML research in the following.
These explanations of the strategies will be supported by a simple running example.\newline
The unconstrained target function $f_0 \left( \begin{bmatrix}x\\y \end{bmatrix} \right) = x^2 + y^2$, which can be seen in Fig.~\ref{fig:theory:target-function}, shall be minimized.
This function is defined in $\mathbb{R}^2$ and has one global minimum.
Although the running example is a minimization task, the presented algorithms can also perform maximization tasks out of the box or with minor modifications.\newline
This function with only two dimensions would probably not be the target function of any AutoML problem and since it is derivable and convex, there is actually no need to apply a black box optimization algorithm.
However, this function can be illustrated clearly and has not a complex shape and applying optimization algorithms on it is therefore easier to understand than a more realistic function.
\begin{figure}[ht!]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \begin{tikzpicture}
            \begin{axis}[
                xmin=-1,xmax=1,ymin=-1,ymax=1,
                width=\textwidth,
                xlabel = {x},
                ylabel = {y},
                xtick = {-0.5,0,0.5},
                ytick = {-0.5,0,0.5},
                ztick = {0.5,1,1.5},
                tick label style={font=\tiny},
                label style={font=\tiny}
            ]
                \addplot3[
                    surf,
                    opacity=0.5,
                    samples=25, samples y=25,
                    domain=-1:1, domain y=-1:1
                ]
                {x^2+y^2};
            \end{axis}
        \end{tikzpicture}
    \end{subfigure}
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{gfx/Figures/Theory/OptimizationTargetFunctionContour.png}
    \end{subfigure}
    \caption[A two dimensional optimization target function.]{A two dimensional optimization target function. It has a global minimum at \ensuremath{\begin{bmatrix} 0\\0 \end{bmatrix}}. Displayed as a surface plot on the right and on the left as a contour plot.}
    \label{fig:theory:target-function}
\end{figure}

\subsection{Optimization by Searching}
\label{sec:theory:optimization:search}
Optimizing with a search will evaluate candidates with the concept of \textit{Neighborhood} between candidates.
Beginning with a single starting candidate, the search will extend its set of evaluated candidates by selecting one candidate $a$ out of this set and evaluating one candidate out of the set of neighboring candidates of $a$.\newline
The concrete definition of this neighborhood concept is depending on the examined search space, which is constructed from the optimization candidate space, and the applied search algorithm.
For example, a neighbor could be any candidate within a certain distance of another candidate according to a given distance metric, or a graph could be created out of a set of candidates and neighbors of a candidate are the adjacent candidate nodes in the graph.\newline
Which neighbor should be selected for evaluation next, and therefore defining the order of candidate evaluations implicitly, can vary with the different approaches as well.
It could be completely random, pre-defined before the search starts, or based on a heuristic scoring of the candidates.

A wide range of search algorithms was developed in the corresponding research area and with a suitable neighborhood concept, most of them are applicable to optimization problems as well.
Some state-of-the-art AutoML approaches use search algorithms as their optimization method.
Three of these search algorithms are explained in more detail with the aid of the running example in the following.

\subsubsection{Random Search}
\label{sec:theory:optimization:search:random}
The random search algorithm comes in two variants.
In the first one, the candidates are drawn completely at random out of the whole candidate space and evaluated.
When the optimization budget is spent, the best candidate is returned as the result.
Since this method does not use any knowledge gathered during the optimization, it depends on chance and the optimization budget if the optimal solution or at least a near optimal solution is found.\newline
In the second one, a starting candidate is drawn at random out of the candidate space and evaluated.
The next candidate will be randomly drawn out of a hypersphere with a pre-defined radius $r$ around the starting candidate and evaluated, i.e. for all dimensions, if the value $z$ is the current value for this dimension, it has to be from the range $[z-r, z+r]$.
If the drawn candidate has a better evaluation score than the starting candidate, the next candidate will be drawn from a hypersphere around the second candidate and from the hypersphere of the starting candidate otherwise.
The loop is continued until the optimization budget is spent.
By choosing a large $r$ the risk of jumping over an optimum increases, but with a low $r$ the coverage of the search of the optimization space will be low as well.\newline
However, this method is very prone to local minima/maxima.
This can be circumvented to a certain degree by stopping the loop of sampling from the hypersphere around the current best candidate if there was no improvement during the latest $n$ iterations.
In this case, the currently searched area is disregarded and the loop is started over with a completely new random start point.
Both variants are illustrated for the running example in Fig.~\ref{fig:theory:random-search}.\newline
One example of the random search algorithm applied in the context of AutoML is \textit{Hyperband}~\cite{Li-Hyperband}.
In the beginning, a big set of candidates is randomly chosen to have a high chance for big coverage of a joint optimization space of model selection and model configuration.
Therefore, this is basically multiple steps of the first random search variant at once.
These chosen candidates are evaluated in parallel and the single evaluations are iteratively terminated, when it becomes evident that the corresponding candidate is not competitive with other currently evaluated candidates.
Selecting candidate evaluations for termination is tackled with concepts from Multi-Armed Bandit problems.
Each evaluation instance is treated as the arm of a bandit and single instances are terminated over time, if they do not yield competitive first results during the evaluation.
The probability that this candidate will improve significantly later in the evaluation process is low and therefore there is a low regret when terminating and not completing the candidate evaluation.\newline
After freeing a part of the optimization budget that was allocated for a candidate evaluation by terminating it, the gained budget can be used for a new randomly chosen candidate.
This new candidate is added to the pool of evaluation instances as a new bandit arm.
By terminating some probably low performing evaluations, the optimization budget is not wasted on fully evaluating candidates from low performing areas and increasing the chance for a good optimization space coverage instead with more evaluations.
Strategies like this one are often referred to as \textit{Early Stopping}.
In the concrete context of Hyperband, selecting the instances, which will not be stopped early, is done in a non-stochastic manner with the \textit{Successive Halving} algorithm~\cite{Jamieson-SuccessiveHalving}.
\begin{figure}[ht!]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{gfx/Figures/Theory/RandomSearch.pdf}
    \end{subfigure}
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{gfx/Figures/Theory/RandomSearchHypersphere.pdf}
    \end{subfigure}
    \caption[Visualization of different variants of random search strategies.]{Visualization of different variants of random search strategies. On the left is the first variant with a few randomly selected candidates. On the right are six possible first iterations of the second variant. The hypersphere is only shown for the first candidate.}
    \label{fig:theory:random-search}
\end{figure}


\subsubsection{Grid Search}
\label{sec:theory:optimization:search:grid}
A grid search tries to cover with a na\"ive brute-force approach as much of the candidate space as possible.
From each dimension of the candidate space, a few values are selected either manually, or more commonly in periodic intervals from the different dimensions.
For example, if the x-dimension of the running example should be evaluated in the range $[-1,1]$, depending on the available optimization budget, potential value selections for this dimension could be $\{-1,0,1\}$ or $\{-0.75,-0.25,0.25,0.75\}$.
With this method the candidate space is discretized and therewith becomes enumerable.\newline
After selecting the value sets for each dimension, the Cartesian product of all sets is calculated.
The elements of this Cartesian product, which are valid candidates because they have a value of each dimension, are the candidates that will be evaluated during the search and for the order of evaluations, any arbitrary sequence of the set elements can be selected.
After either all candidates from the Cartesian product are evaluated or all of the optimization budget is spent, the best evaluated candidate is the overall optimization result.
An illustration of a grid search over the candidate space of the running example can be seen in Fig.~\ref{fig:theory:grid-search}.\newline
Therefore, as opposed to random searches where by chance only a small portion of the candidate space could be covered, a certain coverage of the space is assured.
Especially in contrast to the second variant of the random search, the advantage is that there is no risk of getting stuck in a local optimum.
But in comparison with the second random search variant, the main drawback of a grid search comes clear as well: A grid search has pre-defined candidates that are homogeneously distributed in the space, including the areas with a low performance, and a previously good evaluated candidate does not have any influence on the evaluation candidate set or evaluation order.
This implies that even if the grid search would evaluate a candidate comparably close to a global optimum, it would completely disregard this chance and not continue the search close to this candidate to potentially find this global optimum.\newline
Grid searches are as well as random searches, classical algorithm choices for hyperparameter optimization because they are easy to implement.
In the context of AutoML, a 2-dimensional grid search was utilized in the Weka toolbox~\cite{Witten-Weka} for a simple AutoML approach.
There, a joint candidate space was searched with a set of classifiers for model selection on one dimension and the other dimension is used to select the number of components of Partial Least Squares filter, which is used as a pre-processor, and can therefore be considered as a simple model configuration.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=\textwidth]{gfx/Figures/Theory/GridSearch.pdf}
    \caption{Illustration of a grid search covering the search space of the running example systematically.}
    \label{fig:theory:grid-search}
\end{figure}


\subsubsection{(Heuristic) Graph Search}
\label{sec:theory:optimization:graph}
While a random search does it leave to a certain degree to chances which candidates are evaluated, the candidate space will not be adequately covered with a high probability.
A grid search will achieve a certain coverage guarantee by default, depending on the number of selected values per dimension.
But on the other hand, a grid search will not utilize any gathered knowledge about the candidate space and searches it completely homogeneously while at least the second variant of a random search is capable of utilizing knowledge about one prior evaluation.\newline
A graph search, usually in combination with a heuristic for search guidance, tries to achieve a trade-off between the advantages of both.
Via the transformation to a graph, it has a discretized view of the optimization space just as a grid search and with such an underlying structure, the coverage of the space can be increased.
But by utilizing the knowledge gathered during the search, depending on the search algorithm and the heuristic, often to a greater degree than a random search, it circumvents the homogeneous search of the complete space of grid searches.\newline
As the name suggests, for a graph search the candidate space is depicted as a graph, but how the space is transformed into a representation consisting of nodes and edges is very domain and approach specific and several methods exist.
One exemplaric method could be to use a tree as a search graph, where only the leaf nodes are concrete candidate nodes, which can be evaluated.
Here, the inner nodes are used to progressively discretize the candidate space dimensions hierarchically by splitting the range into multiple intervals and creating a new child node for each one.
This is continued until the intervals of each dimension either only consist of one element or are small enough such that one value out of it can be selected arbitrarily.
At this point, a leaf node would be the child node because for each dimension a value is selected such that the candidate's configuration is complete and could be evaluated.\newline
With a given start node and a method to expand this start node incrementally into a bigger graph, a large number of search algorithms exists to search this graph for the best candidate.
They mostly differ in the selection method deciding which unexplored node should be visited next.
The set of nodes, consisting of the unexplored neighbors of already visited and expanded nodes, is sometimes called a \textit{Search Front} and it is exemplarily illustrated for a search tree in Fig.~\ref{fig:theory:graph-search-front}.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=\textwidth]{gfx/Figures/Theory/GraphSearchFront.pdf}
    \caption[Search graph of a graph search in the middle of an ongoing search.]{Search graph of a graph search in the middle of an ongoing search. After a few iterations of a search algorithm beginning at the root node (R), a few nodes are already explored and expanded, here illustrated in blue. The expanded child nodes of the searched area(blue) represent the search front(grey nodes) and one of them will be explored and expanded in the next search iteration.}
    \label{fig:theory:graph-search-front}
\end{figure}

Some search algorithms select nodes out of the search front  based on the point in time, where they were added to the search front, as for example a Depth-First Search or a Breadth-First Search.
But to achieve a purposeful search, the node selection should not depend on the insertion order of the search front.
Instead, the next node out of the search front should be the node, were the best evaluation score or a path to the best evaluation score can be expected.\newline
If each node would exactly represent one candidate, this is trivial because each node can be evaluated immediately.
But for the aforementioned graph creation method, this is more challenging because only leaf nodes can be evaluated and not inner nodes.
To assign a score to an inner node without searching the complete sub-graph below it, an estimation method is utilized, usually called a \textit{heuristic}.

For example with some available domain knowledge, if certain discretization intervals or selected values are more promising than others, the heuristic can assign higher scores to the corresponding child nodes representing the interval or value.
In the AutoML context, this could be done if for example a certain pipeline component performed outstandingly for previous experiments.
But usually, this will not be the case because the relationships between pipeline components to each other and their parameter configuration is too complex and even minor changes can have a big influence on the evaluation score.
Therefore, it is hard to score an incomplete pipeline even with pre-existing domain knowledge.\newline
Without a way to deterministically score an inner node, a probabilistic approach is more promising.
Here, it can be assumed if enough leaf nodes at the end of the sub-graph below the inner node, which shall be scored, are evaluated, it is possible to score the inner node based on these evaluations with sufficient certainty.
Which path to follow to reach a leaf node in the sub-graph is decided randomly.
To score something based on a series of random experiments is called a \textit{Monte-Carlo} simulation/experiment and is a common approach to obtain a value, where the exact calculation of the value is either impossible or too complex.
This combination of utilizing a heuristic graph search with Monte-Carlo simulations as a heuristic for optimization in the AutoML context was applied to a Best-First Search in the ML-Plan approach~\cite{Mohr-ML-Plan}.

\subsection{Genetic and Evolutionary Optimization}
\label{sec:theory:optimization:genetic}
Genetic and evolutionary optimization methods are inspired by the process of the evolution in nature, as described in Charles Darwins \textit{Natural Selection} theory.
Exactly like a biological evolution model for a species, the optimization process is modeled in consecutive generations and each generation consists of a high number of individuals.
Ideally, from an optimization point of view, only the individuals that are most adopted to their surroundings should survive and produce offspring and therewith influence the succeeding generation, which is based on these individuals with minor changes.
This is often referred to as \textit{survival of the fittest}.\newline
Transferred to optimization, this would mean that each individual is a possible solution candidate and therefore a generation is a set of solution candidates.
Starting with a base generation of usually randomly created individuals, a genetic algorithm follows this pattern:
\begin{enumerate}
    \item Evaluate each individual of the current generation.
    \item Select a subset of the current generation that will be used to create the next generation. Normally, this subset is created as a mix of the highest scoring individuals as well as some randomly selected individuals to prevent focussing on local optima. But other selection methods were also developed.
    \item Generate the next generation based on the selected subset. A pre-defined ratio of individuals will be directly copied into the next generation and the remaining individuals will be modified with evolutionary operators to attempt an enhancement.
\end{enumerate}
These steps are usually repeated until either a certain target score is achieved with one individual, the optimization budget is spent, or the scores did not improve over the last $n$ generations.

The aforementioned evolutionary operators work on so called \textit{genetic representations} of candidates, sometimes also called \textit{genomes} or \textit{chromosomes}.
In theory, nearly every data structure is suitable for this representation but most often simple arrays or vectors are used, where each cell represents one property of the candidates or alternatively the value of one dimension of the candidate space.
For the running example, this would mean an array with length 2, one for the $x$ and one for the $y$ dimension.\newline
Just as in the general principle from biological evolution, these genomes can change by random mutations as well as by recombination, when two individuals produce a new individual.
For a mutation, one individual is selected and one or more cells are changed slightly at random and the other values are kept the same.
If the number of cells to change is equal to the length of the array or vector, this is the same idea of randomly selecting a candidate out of hypersphere around another candidate already mentioned for random searches.
But especially in combination with the recombination operator, the positions of the individuals in the optimization space can be re-organized as new combinations to increase the chance of a high candidate space coverage.\newline
A recombination based on two genomes $g_1$ and $g_2$ to produce a new genome $g^*$ is usually performed as a cross-over.
For each cell of $g^*$ the value is taken from either $g_1$ or $g_2$.
Depending on the type of cross-over, there are $k$ cross-over points, with $k \in [1 ... |g^*| - 1]$.
The points themselves are usually selected randomly.
Each cross-over point marks an index of $g^*$ at which it will be switched from $g_1$ to $g_2$ or vice versa from which source genome the next value will be taken from.
An example for a two-point cross-over is shown in Fig.~\ref{fig:theory:crossover}.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.6\textwidth,keepaspectratio]{gfx/Figures/Theory/Crossover.pdf}
    \caption{An example of a two-point cross-over to produce genome $g^*$ from the genomes $g_1$ and $g_2$.}
    \label{fig:theory:crossover}
\end{figure}

With the random factors during the selection of the individuals for the creation of the next generation as well as during the application of the evolutionary operators, genetic and evolutionary optimization approaches are a stochastic optimization method just as a random search or some graph search algorithms with certain heuristics.
Therefore, the chances of finding a high scoring solution, the coverage of the candidate space, as well as the risk of ending in a local optimum are highly dependent on the number of iterations the algorithm performs as well as other configuration parameters as for example the population size.

Additionally to the challenges of handling chances and probabilities emerging from any probabilistic approach, in the context of AutoML there is one more major challenge in applying a genetic algorithm for optimization.
A more complicated data structure then arrays or vectors is needed to represent a pipeline configuration as a single genome because usually the pipelines do not have a maximal length and can be composed as an arbitrary graph or tree, which cannot be represented in an array with a maximal length.
Even if a suitable data structure is found, the mutation and cross-over implementations have to be made very cautiously, because representations of pipelines can be very error-prone.
Already small changes to one representation value can lead to an invalid pipeline construction that cannot be instantiated.\newline
Two state-of-the-art approaches tackling these challenges and applying genetic algorithms as optimizers for AutoML problems are \textit{TPOT}~\cite{Olson-Tpot}, using expression trees as a data structure for genetic programming, and \textit{RECIPE}~\cite{Guimar-Recipe}, using grammar-based genetic programming.
Both utilize specialized genetic operators, that acknowledge the complex structure of individuals representing AutoML candidates, such that mutations and cross-overs produce fewer invalid pipelines.

\subsection{Bayesian Optimization}
\label{sec:theory:optimization:bayesian}
Similar to some heuristic graph searches and genetic algorithm implementations, Bayesian optimization tries to utilize the knowledge from previous candidate evaluations as much as possible.
The goal is to select the most promising candidate for the next evaluation and therefore minimize the overall number of evaluations until a near-optimal score is reached.
Since each evaluation in the context of AutoML comes with assembling a candidate pipeline and usually performing a cross-validation of the pipeline on the dataset, each evaluation consumes an often not insignificant portion of the optimization budget.
Therewith, Bayesian Optimization is very well suited for black box optimization in the context of AutoML.\newline
The notation of a \textit{most promising} candidate could be interpreted in different ways.
For example, a candidate near the current optimum, which has a high probability of having a comparable or even better score, or a candidate in a completely unevaluated region of the candidate space, where therefore an optimum could lie unnoticed.
But in the case of Bayesian Optimization, this notation of most promising is more sophisticated.

In general, the goal is to create a surrogate function model that replicates the unknown target function as closely as possible with the aid of Bayesian statistics.
To model the Bayesian surrogate, a common choice is a regression via a Gaussian Process.
Detailed information about Gaussian Processes can be found for example in~\cite{Rasmussen-Gaussian-Processes}.\newline
This surrogate is based on the results of several evaluations as samples and will become more precise with each new sample, which is usually the next most promising candidate.
For the actual selection of the next most promising candidate sample, a so called \textit{Acquisition Function} is used and the next value will be at an optimum of this acquisition function.
The value of such an acquisition function for a candidate is based on two factors:
\begin{enumerate}
    \item The estimated score of the candidate, which is based on the modeled Bayesian surrogate
    \item The size of the credible interval (basically the concept of confidence intervals in the context of Bayesian statistics) of the Bayesian surrogate model at the position of the candidate
\end{enumerate}
The size of the credible interval is especially important because if at a point this interval is very broad, the chances are higher that the value of the target function can differ widely from the surrogate at this position.
This would indicate a higher uncertainty of the model.
Of course, the value differ can in both directions, but this can be examined with an additional sample at this position.
Therefore, if the estimated performance already indicates an optimum and additionally the credible interval is broad (i.e. an optimum of the acquisition function based on these two factors), the value of the target function could be even more optimal at this position and it is the most promising candidate to be drawn as the next sample.

For candidates that were already evaluated, this acquisition score is set to zero because the target score at this position is already known and the surrogate can be certain at this position.
A graphical intuition for the concept of acquisition functions is given in Fig.~\ref{fig:theory:acquisition-function}, where Bayesian optimization on one dimension of the running example is illustrated.
\begin{figure}[ht!]
    \centering
    \begin{subfigure}{\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                xmin=-1, xmax=1,
                ymin=0, ymax=1,
                domain=-1:1,
                xlabel=$x$,
                ylabel=$f_0 \left( \begin{bmatrix}x\\0 \end{bmatrix} \right) $,
                no markers,
                height=5cm,
                width=0.75\textwidth,
            ]
                \addplot[lime]{x^2};
            \end{axis}
        \end{tikzpicture}
        \caption[.]{Slice of the target function $f_0$ at $y=0$.}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                xmin=-1, xmax=1,
                ymin=-0.2, ymax=1,
                domain=-1:1,
                xlabel=$x$,
                ylabel=$f_s \left( \begin{bmatrix}x\\0 \end{bmatrix} \right) $,
                height=5cm,
                width=0.75\textwidth,
            ]
                \addplot [only marks,red] coordinates {(-0.7,0.49) (-0.5,0.25) (-0.2,0.04) (0.4,0.16) (0.85,0.7225)};
                \addplot [,
                    smooth,
                    no marks,
                    solid,
                    line join = round,
                    red
                ] coordinates {(-1,0.875) (-0.7,0.49) (-0.5,0.25) (-0.2,0.04) (0.1,0.05) (0.4,0.16) (0.7,0.5) (0.85,0.7225) (1,0.9)};
                \addplot [,
                    smooth,
                    no marks,
                    dashed,
                    line join = round,
                    red
                ] coordinates {(-1,0.675) (-0.85,0.58) (-0.7,0.49) (-0.56,0.46) (-0.5,0.25) (-0.4,0.05) (-0.2,0.04) (0.1,0.2) (0.4,0.16) (0.7,0.29) (0.85,0.7225) (1,0.99)};
                \addplot [,
                    smooth,
                    no marks,
                    dashed,
                    line join = round,
                    red
                ] coordinates {(-1,1.075) (-0.85,0.79) (-0.7,0.49) (-0.64,0.31) (-0.5,0.25) (-0.35,0.21) (-0.2,0.04) (0.1,-0.11) (0.4,0.16) (0.55,0.58) (0.85,0.7225) (1,0.81)};
            \end{axis}
        \end{tikzpicture}
        \caption[.]{
            Estimated surrogate function $f_s$ modeled after 5 samples.
            The evaluated samples are shown as red dots, the surrogate function as a red line, and the credible intervals as the two red dashed lines.
            Important note: The values of the surrogate function and the credible intervals are not calculated but are only rough estimates to give an intuition of the method.
            The exact values can vary significantly based on the applied kernel of the Gaussian Process.
        }
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                xmin=-1, xmax=1,
                ymin=-0.5, ymax=0.1,
                domain=-1:1,
                xlabel=$x$,
                ylabel=$f_a \left( \begin{bmatrix}x\\0 \end{bmatrix} \right) $,
                height=5cm,
                width=0.75\textwidth,
            ]
                \addplot [,
                    smooth,
                    no marks,
                    solid,
                    line join = round,
                    blue
                ] coordinates {(-1,-0.1) (-0.85,-0.075) (-0.7,0)};
                \addplot [,
                    smooth,
                    no marks,
                    solid,
                    line join = round,
                    blue
                ] coordinates {(-0.7,0) (-0.6,-0.2) (-0.5,0)};
                \addplot [,
                    smooth,
                    no marks,
                    solid,
                    line join = round,
                    blue
                ] coordinates {(-0.5,0) (-0.35,-0.25) (-0.2,0)};
                \addplot [,
                    smooth,
                    no marks,
                    solid,
                    line join = round,
                    blue
                ] coordinates {(-0.2,0) (0.1,-0.4) (0.4,0)};
                \addplot [,
                    smooth,
                    no marks,
                    solid,
                    line join = round,
                    blue
                ] coordinates {(0.4,0) (0.625,-0.225) (0.85,0)};
                \addplot [,
                    smooth,
                    no marks,
                    solid,
                    line join = round,
                    blue
                ] coordinates {(0.85,0) (0.925,-0.0485) (1,-0.06)};
            \end{axis}
        \end{tikzpicture}
        \caption[.]{
            Acquisition function $f_a$ based on $f_s$ and its credible intervals after the 5 drawn samples.
            As in (b), the values are only estimates and not explicitly calculated.
            The minimum of $f_a$ at $x=0.1$ would be the best choice for the sixth sample.
            There the expected value of the surrogate regression is already low and the credible interval is additionally broad.
        }
    \end{subfigure}
    \caption[Simple illustration of acquisition functions for Bayesian optimization.]{Simple illustration of acquisition functions for Bayesian optimization.
        It is applied to the running example of minimizing \ensuremath{f_0 \left( \begin{bmatrix}x\\y \end{bmatrix} \right) = x^2 + y^2}.
        For simplicity will the Bayesian optimization only be used to minimize on the x-dimension and y will be set to 0, but in theory can this approach be extended to any number of dimensions.
        (a) shows a slice of the target function in green.
        (b) shows in red the already evaluated samples, the estimated surrogate function model, and the credible intervals.
        (c) shows in blue the acquisition function based on the values in (b).
    }
    \label{fig:theory:acquisition-function}
\end{figure}

An exemplaric algorithm for Bayesian Optimization is given by~\textcite{Frazier-Bayesian-Optimization} with the following pseudo-code in Algorithm~\ref{alg:bayesian-optimization}.
\begin{algorithm}[ht!]
    \SetAlgoLined
    \DontPrintSemicolon
    Place a Gaussian process prior on $f$\;
    Observe $f$ at $n_0$ points according to an initial space-filling experimental design\;
    Set $n \leftarrow n_0$\;
    \While{$n \leq N$}{
        Update posterior probability distribution on $f$ using all data\;
        Let $x_n$ be a maximizer of the acquisition function over $x$, where the acquisition function is computed using the current posterior distribution\;
        Observe $y_n \leftarrow f(x_n)$\;
        Increment $n$\;
    }
    Return a solution: either the point with the largest $f(x)$, or the point with the largest posterior mean\;
    \caption{Basic pseudo-code for Bayesian optimization}
    \label{alg:bayesian-optimization}
\end{algorithm}

The functionality of this pseudo-code as well as some technical terms are explained in the following line by line:
\begin{enumerate}
    \item A probability distribution, for example a Normal distribution is selected as an initial prior distribution $f_0$ to further construct the surrogate function with.
    \item Since this initial $f_0$ is not very meaningful regarding the target function, a few candidates will be evaluated before the actual optimization loop starts.
    These candidates are selected to be a space-filling design, which means to have the best possible coverage of the candidate space.
    Therewith, the credible intervals between the initially observed points can be kept as small as possible before the acquisition function, which uses these intervals, is used.
    An example of a simple space-filling experiment design is to choose candidates uniformly distributed in the candidate space.
    \item Since this implementation uses the number of observations of $f$ for a given point, i.e. candidate evaluations, as an optimization budget, the variable $n$, which keeps track of the spent budget, is set to $n_0$, because this is the number of initial evaluations.
    \item Here starts the main optimization loop.
    As long as the optimization budget is not spent and therefore $n \leq N$, further candidates can be selected and evaluated.
    \item Let $d_n$ be all evaluated points from previous iterations or from $n_0$, the surrogate function model of this iteration $f_n$ will be updated based on $d_n$.
    Therefore, a new surrogate function posterior probability distribution is calculated by updating the chosen distribution with the previous posterior distribution $f_{n-1}$ as a new prior, i.e. $f_n := f(x)|f_{n-1}$.
    \item $x_n$ is selected to be the candidate of this iteration for evaluation. It is chosen by searching the acquisition function, which was updated with the data from prior iterations, for an optimum.
    \item Get an evaluation score $y_n$ for the selected candidate of this iteration $x_n$ by observing $f(x_n)$, thus having one more data point to base to posterior probability distribution on and to construct a more precise surrogate function $f_{n+1}$ in the next iteration.
    \item Increment $n$ because another candidate evaluation was performed and therefore the remaining optimization budget decreased.
    \item The optimization budget is spent and the main optimization loop will terminate.
    \item After the main optimization loop finishes, the algorithm will terminate and return a candidate as the result.
    It will select the best candidate among the evaluated points so far and compare it to the point with the largest posterior mean, i.e. the point where the global optimum, based on the overall constructed surrogate function $f_N$, is assumed.
    If this point is assumed to be better than the best evaluated candidate it will be returned and the best evaluated candidate otherwise.
\end{enumerate}
A state-of-the-art implementation of Bayesian optimization for algorithm configuration problems is \textit{SMAC}~\cite{Hutter-SMAC}, an abbreviation for Sequential Model-Based Optimization for General Algorithm Configuration.
It is utilized as an optimization approach for a joint model selection and model configuration AutoML optimization space by several approaches, including \textit{Auto-WEKA}~\cite{Thornton-AutoWeka} and \textit{auto-sklearn}~\cite{Feurer-AutoSklearn}.

\subsection{No-Free-Lunch Theorem}
\label{sec:theory:optimization:lunch}
The state-of-the-art AutoML approaches, presented with their corresponding black box optimization method, have one thing in common: They all apply a single optimization algorithm to find the best candidate from an optimization space, which is a joint model selection and model configuration space.
While this is a valid approach and will find an optimal or near optimal solution for many AutoML problem instances, it is theoretically impossible that the best solution can be found with a single optimization method for every AutoML problem instance, if it is a setting with constraints as for instance an optimization budget.\newline
This can be deduced from the \textit{No-free-lunch Theorems}, which were proven for optimization problems by~\textcite{Wolpert-No-Free-Lunch-Theorems}.
In these theorems, it is stated that when an optimization algorithm performs superior for one problem or class of problems, it has to pay for this by performing inferior for other problems.
Of course, this is only the case for optimizing with an optimization budget or other constraints because the most optimization algorithms can find the best solution if they have an unlimited budget, even if it is just by evaluating each possible candidate in a brute force manner.

For the context of the AutoML setting, the notion of a problem class could for example be transferred to the data, which is given as an input.
Therefore, in this setting with a limited budget of time or resources, one single optimization algorithm may find the best pipelines and configurations for some datasets but cannot achieve this for all datasets.
Thus, if the so far presented optimization approaches would be applied to an AutoML problem, they would be inherently incapable of being the superior approach for all datasets.
With a given dataset $D_1$ a genetic algorithm may find the best solution, while a Bayesian optimization method outperforms the same genetic algorithm for another dataset $D_2$.

It is additionally possible that the quality of an optimizer does not only depend on the input dataset, i.e. the AutoML problem class instance, but additionally on the concrete optimization space it operates on and the optimization budget.
These three components could be used to construct a specific problem instance for a dataset problem class.\newline
If this optimization space is modified and thus a new problem instance $p_2$ is created from the original problem instance $p_1$, this can have a huge influence on the optimizer qualities and the optimization method that was the most successful one for $p_1$ could now theoretically be the worst one for $p_2$.

Such a modification of the optimization space can be created if, for instance, the model selection part is already finished and the remaining model configuration step induces a new optimization space, which is now the problem of optimizing the parameters of a concrete pipeline.
Here, different optimization spaces can be created out of one initial space by pre-selecting a pipeline and having the pipeline hyperparameters define the remaining optimization space.\newline
Now, different optimizers can differ in quality here as well solely based on the selected pipeline components.
For example, with a given dataset, optimizer $A$ could find good configurations for a pipeline consisting of a Principal Component Analysis followed by a Decision Tree, while optimizer $B$ yields better results in the case of a single Support Vector Machine.
The same transformations of the optimization space can of course be done by only considering the model selection instead and not the model configuration.

Since one optimization method will only be the best one for a limited set of problem classes, it can be beneficial to use more than one optimization method and utilize a different optimizer for different transformations of the optimization space.
Therewith, the the set of problem classes, where a combination of these optimizers can now be superior, is extended.\newline
In the next section, a selection of approaches will be presented, which use such transformations of the optimization space and more than one optimizer, to utilize this extension of problem classes in the AutoML context, where the theoretical superiority can be achieved by multiple optimizers.

\section{Related Work}
\label{sec:theory:related}
The first presented approach that uses different optimizers for different transformed AutoML optimization spaces is \textit{ReinBo}~\cite{Sun-ReinBo}.
Here, the overall optimization space, including model selection and model configuration, is transformed into a solely model selection space at first.
For practicability, each pipeline in this approach can only consist of up to three components: One for data pre-processing, one for feature engineering, and one as the machine learning algorithm.
The model selection is performed in three stages, in each of which one of the three component types is selected successively.
It is also possible to not select any component for a pre-processing or feature engineering, which could be represented by a symbol as for example "NA".
A simple example for a small optimization space for such a model selection method would therefore consist of three sets:
\begin{itemize}
    \item A set $S_p$ with pre-processing algorithms: For example $S_p=\{$Min-Max Scaling, One-Hot Encoding, NA$\}$
    \item A set $S_f$ with feature engineering methods: For example $S_f=\{$ PCA, Variant Threshold Filtering, NA$\}$
    \item A set $S_m$ with machine learning algorithms: For example $S_m=\{$ SVM, Decision Tree$\}$
\end{itemize}
Such a model selection space as in ReinBo can be imagined as a tree, where the root is an empty pipeline that has none of the three component types selected.
This root node has a child node for each $s_p \in S_p$, including the representation symbol "NA" for not selecting any pre-processing component.
Therefore, the child node which represents one $s_p$ means that the pipeline from this subtree uses $s_p$ as a pre-processor.
Accordingly, such a child node for an $s_p \in S_p$ has a child node itself for each $s_f \in S_f$ and ultimately each $s_f$ node has a child node as well for each $s_m \in S_m$, which now is a leaf node since the pipeline components are completely constructed.
An illustration of the previous examples optimization space can be seen in Fig.~\ref{fig:theory:reinbo-model-selection}.
\begin{figure}[ht!]
    \centering
    \includegraphics[width=\textwidth,height=0.7\textheight,keepaspectratio]{gfx/Figures/Theory/ReinBoModelSelection.pdf}
    \caption{Simple visualization of the underlying concepts of the three layer ReinBo model selection candidate space.}
    \label{fig:theory:reinbo-model-selection}
\end{figure}

Because of the tree-shaped structure of this optimization space, a hierarchical reinforcement learning approach is trained during the optimization.
With learning a policy for suitable pipeline component choices, it can act as an optimizer for model selection.\newline
After the reinforcement learning algorithm has selected up to three pipeline components, the possible parametrization of each component is examined.
From the possible parametrization of each selected component, a model configuration space can be induced.
Therewith, based on a completed model selection, the overall optimization space is once more transformed, but now to a space representing possible model configurations for the constructed pipeline.
Each leaf node of the tree structure has such a small model configuration space.\newline
Once the reinforcement learning algorithm performed the action of selecting a component from the last hierarchical layer and it reaches such a leaf node, Bayesian optimization is conducted in the model configuration space of this leaf.
The evaluation result of the best candidate of this optimization run is given as a reward to the reinforcement learning algorithm and it can update its policy.
Afterwards, in the next iteration, the reinforcement learning algorithm starts once again at the root node and has to select pipeline components based on the policy and this is repeated until the optimization budget ist spent.\newline
Because Bayesian optimization is not very effective for high dimensional spaces, it is a suitable adjustment for an AutoML approach to not run a Bayesian optimization on the complete candidate space but on a smaller transformed space.
Instead of Bayesian optimization, the model selection is performed by another optimizer and only a small portion of the model configuration space is the input for a Bayesian optimization run, which includes only the hyperparameters of the selected components.
The dimensionality is reduced significantly and Bayesian optimization will be more effective.\newline
But there is still a big amount of problem classes where the Bayesian optimization method itself can not be the suitable optimization algorithm choice.
Therewith is the set of problem classes, where this approach can be superior, not extended drastically by the improvements regarding the reduction of dimensions of the Bayesian optimization space and the inclusion of a reinforcement learning algorithm as an additional optimizer.

The advantages of performing the AutoML optimization in several phases on transformed optimization spaces was further examined by~\textcite{Quemy-Two-Stage-Optimization}.
Besides the aforementioned opportunity to compensate one less suitable optimization approach with another possibly better suited optimizer, in this publication additionally was stated, that if the optimization space is transformed into two or more smaller sub-spaces, the overall optimization process can be sped up.
There, a two-stage optimization transformation was applied, alike ReinBo separating model configuration and model selection.
Although, this approach is not as limited to a mandatory pipeline topology as ReinBo, with the constraint of three components at most and only one of each type, and can construct pipelines in a more unrestricted fashion as a directed acyclic graph.
To create an optimization space of all valid pipeline graphs, all pipeline components have specifications of their input and output datatypes and can only be connected to another component node, if the output types of one node match the input types of another node.\newline
In the formal descriptions of this approach, no specific optimization algorithm is proposed.
For this framework, any optimization algorithm could be applied for either phase or the same one for both phases as well.
Instead, Quemy focusses on the allocation of the optimization budget, in their case a time budget $T$, between the two phases.
The following allocation policies were defined:
\begin{itemize}
    \item \textit{Split policy}: $T$ is split into pre-defined phase allocations $T_1$ and $T_2$. $T_1$ is completely spent onto model selection and afterwards $T_2$ is spent as a whole as well.
    \item \textit{Iterative policy}: A very short time period $t << T$ is set and it will be alternated iteratively between the two phases, where each phase has $t$ as a budget in one iteration, until $T$ is spent.
    \item \textit{Adaptive policy}: This policy is an extension of the Iterative policy but here the $t$ is not fix but is adaptive to the achievements of the optimization in each phase.
    Therefore there is a $t_1$ and a $t_2$ for the corresponding phase.
    If in one iteration the evaluation score increases during the optimization of the active phase $i$, $t_i$ is doubled.
    In return, if the phase $i$ did not increase the evaluation score during its last two active iterations, $t_i$ is halved.
    \item \textit{Joint policy}: Here, no separation into different optimization phases takes place and model selection and model configuration is performed in a joint search space.
    Hence, it can spend $T$ completely.
\end{itemize}

Monte-Carlo simulations as a heuristic for a graph search, as explained in section~\ref{sec:theory:optimization:search}, are the basis of a graph search algorithm named \textit{Monte-Carlo tree search}.
In this search, the task of selecting the next child node of one node for expansion is viewed as selecting an arm of a Multi-Armed Bandit problem.
With this approach, a balance between exploring more unevaluated areas of the tree and exploiting subtrees that have already shown promising scores can be achieved.
This balance is achieved via a heuristic scoring of nodes via Monte-Carlo simulations and a node selection strategy based on these scores that is formalized as \textit{UCT}(Upper Confidence bound applied to Trees)~\cite{Kocsis-UCT}.
This combination of UCT and Monte-Carlo simulations is then often referred to as a Monte-Carlo tree search(MCTS).
MCTS will be exemplified in more detail in section~\ref{sec:approach:selection:mcts}.\newline
One example of a heuristic graph search based AutoML approach is called \textit{Mosaic}~\cite{Rakotoarison-Mosaic} and it leverages MCTS employed in a multi-stage optimization methodology as well.
In Mosaic, the tree that is the subject of the MCTS is modeled as a transformation of the optimization space into a model selection space.
The first $k$ layers of the search tree are representing $k$ possible types of pipeline components, for example pre-processing methods or machine learning algorithms.
When a component is selected in layer $i \in [1,k]$ of the tree, it will also be the component with position $i$ in the resulting linear pipeline.
All resulting pipelines are only linear and not generalized to any form of a directed acyclic graph as in the earlier discussed related work.
In each of the first $k$ layers, one component of each type is selected.
Therewith, this approach has a comparable tree structure to ReinBo but is more flexible regarding the length of the pipeline.\newline
After the model selection for a pipeline is concluded, the $k$ components of the pipeline need a configuration.
Thus, the continuous model configuration space that remains of the initial optimization space after the discrete model selection space has been covered with the search tree, is transformed into a sole model configuration space for the selected pipeline, which is embedded in the corresponding leaf node of the selected pipeline at the bottom of the $k$ model selection layers.
Similar to ReinBo, a surrogate model $f_s$ will be constructed in each leaf node.
It is supposed to predict the expected evaluation score for any point of the optimization space, if the pipeline that consists of the components represented by the current leaf is constructed with the parametrization of this point.
The construction of $f_s$ and the optimization is based on Bayesian optimization.


Mosaic and ReinBo have two remaining drawbacks:
\begin{itemize}
    \item They can only create pipelines of a fixed length and are without further extensions not capable of constructing more sophisticated non-linear pipelines, restricting the final pipeline potential for some datasets.
    \item Hyperparameter optimization in the second phase after the model selection is only performed with a single optimization method, namely Bayesian optimization, and is therefore limited by the capabilities of this method.
    But as outlined before with the aid of the No-Free-Lunch theorems, it is possible to improve the set of problem classes, for which an AutoML approach is superior, by utilizing more than one optimization algorithm, if it is possible to automatically detect and exploit the most suitable optimizer for a problem class.
\end{itemize}
The approach of this thesis is similar to the principle of Mosaic and ReinBo but tries to overcome these two drawbacks.
In the following chapter, the functional principles of the approach of this thesis are explained in detail as well as the strategies for both improvements.
